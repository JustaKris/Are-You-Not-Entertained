{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Data Imputation and Preprocessing\n",
    "\n",
    "**Last Updated**: November 2024  \n",
    "**Data Source**: DuckDB Database (`data/db/movies.duckdb`)\n",
    "\n",
    "## Overview\n",
    "This notebook performs comprehensive data imputation and preprocessing for movie revenue prediction modeling. \n",
    "\n",
    "## Modern Data Architecture\n",
    "This project now uses a **modern, database-centric approach**:\n",
    "- **DuckDB** as the primary data source (replacing CSV files)\n",
    "- **Parquet** format for processed data (faster, smaller than CSV)\n",
    "- **Dedicated utilities** in `src/data/query_utils.py` for data access\n",
    "- **Structured directories**: `raw/`, `processed/`, `artifacts/`, `db/`\n",
    "\n",
    "See `DATA_GUIDE.md` for complete documentation.\n",
    "\n",
    "## Workflow\n",
    "1. Load data from DuckDB (joined tables: movies, tmdb_movies, omdb_movies)\n",
    "2. Explore missing data patterns\n",
    "3. Engineer features (awards, dates, multi-label categories)\n",
    "4. Impute missing values using ML algorithms\n",
    "5. Create train/test splits\n",
    "6. Save processed data and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from verstack import NaNImputer\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory config to project root to insure consistency across environments for project specific imports\n",
    "from pyprojroot import here\n",
    "os.chdir(here())\n",
    "\n",
    "# Project specific imports\n",
    "from ayne.core.config import settings\n",
    "from ayne.utils.query_utils import (\n",
    "    load_full_dataset,\n",
    "    get_movies_with_financials,\n",
    "    get_db_client\n",
    ")\n",
    "from ayne.utils.io import (\n",
    "    save_processed_data,\n",
    "    save_artifacts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Data Query Functions\n",
    "\n",
    "The `src/data/query_utils.py` module provides convenient functions for working with the DuckDB database:\n",
    "\n",
    "**Loading Data:**\n",
    "- `load_full_dataset()` - Load all movies with joined TMDB/OMDB/Numbers data\n",
    "- `get_movies_with_financials()` - Filter movies with budget/revenue data\n",
    "- `get_movies_by_year_range()` - Get movies from specific years\n",
    "- `execute_custom_query()` - Run custom SQL queries\n",
    "\n",
    "**Saving Data:**\n",
    "- `save_processed_data()` - Save to `data/processed/` (for analysis-ready data)\n",
    "- `save_artifacts()` - Save to `data/artifacts/` (for model outputs)\n",
    "\n",
    "**Database:**\n",
    "- `get_db_client()` - Get DuckDB connection for custom queries\n",
    "- `get_table_info()` - View table schema\n",
    "\n",
    "All functions return pandas DataFrames and handle connection management automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading from DuckDB\n",
    "\n",
    "The project now uses DuckDB as the primary data source instead of CSV files. This provides:\n",
    "- **Faster queries**: Columnar storage optimized for analytics\n",
    "- **Structured data**: SQL queries for flexible data retrieval\n",
    "- **Single source of truth**: All data consolidated in one database\n",
    "- **Better data types**: No type inference issues from CSV\n",
    "\n",
    "The database contains these tables:\n",
    "- `movies` - Core movie information\n",
    "- `tmdb_movies` - TMDB API data (details, ratings, genres)\n",
    "- `omdb_movies` - OMDB API data (cast, crew, reviews)\n",
    "- `numbers_movies` - Financial data from The Numbers\n",
    "\n",
    "The `load_full_dataset()` function automatically joins all tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from DuckDB\n",
    "# The DuckDB database contains all movie data from TMDB, OMDB, and The Numbers\n",
    "# This replaces the old approach of loading from CSV files\n",
    "\n",
    "try:\n",
    "    print(\"Loading movie data from DuckDB...\")\n",
    "    print(f\"Database location: {settings.duckdb_path}\")  # type: ignore\n",
    "    \n",
    "    # Load complete dataset with all joined tables\n",
    "    data = load_full_dataset(include_nulls=True)\n",
    "    \n",
    "    print(f\"✅ Successfully loaded {len(data)} movies\")\n",
    "    print(f\"Columns: {len(data.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of missing data\n",
    "\n",
    "I have gotten to know this data during the collection process but there are a few aspects I'd like to look into in more detail. Unfortunately, there seems to be quite a bit of missing data, especially in the `budget` and `revenue` columns which I intend to be my main prediction targets when training models. It is not a huge surprise, however, since companies tend to be quite guarded around the financial data of their projects. I will be looking into other sources of financial data in future.\n",
    "\n",
    "Let's have a look at the total amount of missing data. Hopefully the blanks between the two interect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "missing_budget_revenue_count = data[data['budget'].isna() & data['revenue'].isna()].shape[0]\n",
    "missing_budget_count = data['budget'].isna().sum() - missing_budget_revenue_count\n",
    "missing_revenue_count = data['revenue'].isna().sum() - missing_budget_revenue_count\n",
    "\n",
    "# Calculate the total number of rows with missing values in either 'budget' or 'revenue'\n",
    "total_missing_count = missing_budget_count + missing_revenue_count + missing_budget_revenue_count\n",
    "\n",
    "print(f\"- Number of rows where both budget and revenue are missing: {missing_budget_revenue_count}\")\n",
    "print(f\"- Number of additional rows where budget is missing: {missing_budget_count}\")\n",
    "print(f\"- Number of additional rows where revenue is missing: {missing_revenue_count}\")\n",
    "print(f\"Total number of rows with missing values in either budget or revenue: {total_missing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is indeed an unfortunate number of blanks. Most of them intersxect between the two columns but we still end up with about 30-35% of missing between the two so that is something I'll have to work around.\n",
    "\n",
    "Let's have a look at the amount of rows missing financial data throughout time. I have a sneaking suspicion that it will be more common to not report financial data in more recent years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is your DataFrame\n",
    "plot_data = data.copy()\n",
    "plot_data['release_date'] = pd.to_datetime(plot_data['release_date'])\n",
    "\n",
    "# Create a new column to indicate if both budget and revenue are missing\n",
    "plot_data['missing_budget_revenue'] = plot_data['budget'].isna() & plot_data['revenue'].isna()\n",
    "\n",
    "# Group by release year and calculate the proportion of missing values\n",
    "plot_data['release_year'] = plot_data['release_date'].dt.year\n",
    "missing_by_year = plot_data.groupby('release_year')['missing_budget_revenue'].mean()\n",
    "\n",
    "# Plot the proportion of missing values over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(missing_by_year.index, missing_by_year.values, marker='o', linestyle='-')\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Proportion of Missing Budget and Revenue')\n",
    "plt.title('Proportion of Missing Budget and Revenue Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it quite interesting that movie financial data has not been reported as frequently in recent years, especially around the COVID-19 pandemic. The movie industry has grown significantly, and so have the costs of producing films. Nowadays, blockbuster movies are often financed through external companies, making financial details a more sensitive topic. During the pandemic, the movie industry contracted, and the quality of movies declined. Consequently, movies have not been performing well financially of late, and companies like Disney may choose not to report this information to protect their stock value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.dropna(subset=['imdb_votes'])\n",
    "# data['imdb_votes'] = data['imdb_votes'].str.replace(',', '').astype(int)\n",
    "\n",
    "# Limit to the top 20 movies\n",
    "top_movies = data[data['revenue'].isnull()].sort_values(by='tmdb_vote_count', ascending=False).head(20)\n",
    "\n",
    "# Create a pivot table\n",
    "pivot_table = top_movies.pivot_table(index='title', values=['revenue', 'budget', 'tmdb_vote_count', 'release_date'], aggfunc='first')\n",
    "\n",
    "# Sort the pivot table by tmdb_vote_count\n",
    "sorted_pivot_table = pivot_table.sort_values(by='tmdb_vote_count', ascending=False)\n",
    "\n",
    "# Print the result\n",
    "print(sorted_pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilable Categorical Features\n",
    "\n",
    "There are a nuymber of multilable categorical features that will need to be looked into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values_for_feature(df: pd.DataFrame, feature: str, delimiter: str = \",\") -> int:\n",
    "    \"\"\"\n",
    "    Splits the specified feature column by the delimiter and returns the number of unique values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        feature (str): The name of the column to process.\n",
    "        delimiter (str): The delimiter used to separate multiple values in the column.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of unique values.\n",
    "    \"\"\"\n",
    "    return len(df[feature].dropna().str.split(rf\"{delimiter}\\s*\").explode().unique())\n",
    "\n",
    "# List of features you want to analyze:\n",
    "features = [\n",
    "    \"genre_names\", \n",
    "    \"production_company_name\", \"production_country_name\", \n",
    "            \"spoken_languages\", \"director\", \"writer\", \"actors\"]\n",
    "\n",
    "# Create a dictionary with the counts for each feature:\n",
    "unique_counts = {feature: count_unique_values_for_feature(data, feature) for feature in features}\n",
    "\n",
    "# Display the results:\n",
    "for feature, count in unique_counts.items():\n",
    "    print(f\"{feature}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these features contain thousands of unique values, making it impractical to encode them directly. A good approach is to analyze the distribution of observations per category and retain only the most frequent categories. The less frequent categories can be grouped into a single **\"Other\"** category. This method helps to reduce the dimensionality of the data while preserving the most significant information.\n",
    "\n",
    "Let's take a look at the distributions per category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_categories(df: pd.DataFrame, column: str, top_n: int, delimiter: str = \",\", others_label: str = \"Others\") -> None:\n",
    "    \"\"\"\n",
    "    Prints the top_n unique values from a multi-label column and the total count of values \n",
    "    that fall outside the top_n (which would be grouped as 'Others').\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing your data.\n",
    "        column (str): The name of the multi-label column.\n",
    "        top_n (int): The number of top categories to display.\n",
    "        delimiter (str): The delimiter separating multiple values (default is a comma).\n",
    "        others_label (str): The label used for less frequent values.\n",
    "    \"\"\"\n",
    "    # Split the column into individual values and count frequencies\n",
    "    exploded = df[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    \n",
    "    # Get the top N categories and the sum for the rest\n",
    "    top_categories = counts.head(top_n)\n",
    "    others_count = counts[counts.index.difference(top_categories.index)].sum()\n",
    "    \n",
    "    print(\"--------------------------------------------------||\")\n",
    "    print(f\"Top {top_n} unique values for '{column}':\")\n",
    "    print(top_categories)\n",
    "    print(f\"Total count of all other values (will be grouped as '{others_label}'): {others_count}\")\n",
    "    print(\"--------------------------------------------------||\\n\")\n",
    "\n",
    "\n",
    "top_values = {\n",
    "    \"genre_names\": 20,\n",
    "    \"production_company_name\": 20,\n",
    "    \"production_country_name\": 10,\n",
    "    \"spoken_languages\": 10,\n",
    "    \"director\": 20,\n",
    "    \"writer\": 20,\n",
    "    \"actors\": 20\n",
    "}\n",
    "\n",
    "for feature, top_n in top_values.items():\n",
    "    print_top_categories(data, feature, top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `genre_names`: Will be kept as is since the number of unique categories is perfectly manageable.\n",
    "- `production_company_name`: Has a relatively even distribution, making it more useful for specific company investigations rather than model training. Therefore, it will be discarded for model training.\n",
    "- `production_country_name`: Most values are concentrated in the US and the UK, making it a great candidate for consolidating less frequent categories into an \"Other\" category.\n",
    "- `spoken_languages`: Another candidate for consolidation, potentially grouping into the top 5 most common languages.\n",
    "- `director`: Similar to `production_company_name`, it has a wide distribution and will be discarded for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "For this project, I would like to make extensive use of the pipeline functionality in SciKit-Learn to re-familiarize myself with the tool. I will also be making use of `FunctionTransformer` to have custom steps be part of the final pipeline.\n",
    "\n",
    "### NaNImputer from verstack\n",
    "\n",
    "The `NaNImputer` from the `verstack` library is a tool designed to handle missing values in a DataFrame. It provides various strategies for imputing missing values, including simple statistical methods and more advanced techniques. It automates the entire process and makes decisions on its own about the best approach for each column.\n",
    "\n",
    "Due to the nature of the data, each observation in columns like `budget`, `revenue`, and the various critic scores are very individual, and imputation strategies like **median** and **mean** will not be appropriate options. Therefore, I want to make use of machine learning algorithms. `NaNImputer` will make use of `IterativeImputer` for such values, making it a more robust option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add missing indicators for certain columns.\n",
    "def impute_data(df: pd.DataFrame, colums_to_exclude: list = None) -> pd.DataFrame:\n",
    "    if colums_to_exclude:\n",
    "        df = df.drop(columns=colums_to_exclude).copy()\n",
    "    else:\n",
    "        df.copy()\n",
    "    imputer = NaNImputer()\n",
    "    df = imputer.impute(df)\n",
    "    return df\n",
    "\n",
    "imputation_transformer = FunctionTransformer(impute_data, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Types cionversion transformers\n",
    "\n",
    "Some features need to be reformatted and below functions will deal with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns need to get converted to numeric\n",
    "def convert_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        # Convert to string, remove commas, then convert to numeric\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "    return df\n",
    "\n",
    "to_numeric = FunctionTransformer(convert_to_numeric, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing indicator features\n",
    "I want to set up binary features for the columns where I have a large number of blanks to be able to investigate the imputed data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add missing indicators for certain columns.\n",
    "def add_missing_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        df[col + \"_missing\"] = df[col].isnull().astype(int)\n",
    "    return df\n",
    "\n",
    "missing_indicator_transformer = FunctionTransformer(add_missing_indicators, validate=False)\n",
    "\n",
    "# iter_cols = ['metascore', 'rotten_tomatoes_rating', 'meta_critic_rating', 'budget', 'revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "#### Award features\n",
    "The `awards` column can be used to extract various columns for BAFTAs, Oscars and total awards and nominations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_awards_info(awards_str):\n",
    "    \"\"\"\n",
    "    Extracts numerical awards information from a text string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    awards_str : str\n",
    "        The awards description string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series with the following index:\n",
    "        [\"total_wins\", \"total_noms\", \"oscar_wins\", \"oscar_noms\", \"bafta_wins\", \"bafta_noms\"]\n",
    "    \"\"\"\n",
    "    # Handle missing or \"N/A\" values.\n",
    "    if pd.isna(awards_str) or awards_str.strip() in [\"N/A\", \"\"]:\n",
    "        return pd.Series([0, 0, 0, 0, 0, 0],\n",
    "                         index=[\"total_wins\", \"total_noms\", \"oscar_wins\", \"oscar_noms\", \"bafta_wins\", \"bafta_noms\"])\n",
    "    \n",
    "    # Extract overall totals.\n",
    "    # Look for a pattern like \"56 wins\" (we use negative lookahead to avoid picking up Oscar wins)\n",
    "    total_wins_match = re.search(r'(\\d+)\\s+wins?(?!.*Oscars)', awards_str, flags=re.IGNORECASE)\n",
    "    total_noms_match = re.search(r'(\\d+)\\s+nominations', awards_str, flags=re.IGNORECASE)\n",
    "    total_wins = int(total_wins_match.group(1)) if total_wins_match else 0\n",
    "    total_noms = int(total_noms_match.group(1)) if total_noms_match else 0\n",
    "\n",
    "    # Oscar-specific extraction:\n",
    "    oscar_noms_match = re.search(r'Nominated for\\s+(\\d+)\\s+Oscars?', awards_str, flags=re.IGNORECASE)\n",
    "    oscar_noms = int(oscar_noms_match.group(1)) if oscar_noms_match else 0\n",
    "    # Look for something like \"Oscars. 56 wins\" or \"Oscars 56 wins\" (using non-digit separator)\n",
    "    oscar_wins_match = re.search(r'Oscars?[\\W_]+(\\d+)\\s+wins?', awards_str, flags=re.IGNORECASE)\n",
    "    oscar_wins = int(oscar_wins_match.group(1)) if oscar_wins_match else 0\n",
    "\n",
    "    # BAFTA-specific extraction:\n",
    "    # For nominations, sometimes the text might run together (e.g. \"BAFTA Award28 nominations total\")\n",
    "    bafta_noms_match = re.search(r'Nominated for\\s+(\\d+)\\s*BAFTA', awards_str, flags=re.IGNORECASE)\n",
    "    bafta_noms = int(bafta_noms_match.group(1)) if bafta_noms_match else 0\n",
    "    # For wins, allow an optional \"Award\" word after BAFTA.\n",
    "    bafta_wins_match = re.search(r'BAFTA(?:\\s+Award)?[\\D_]+(\\d+)\\s+wins?', awards_str, flags=re.IGNORECASE)\n",
    "    bafta_wins = int(bafta_wins_match.group(1)) if bafta_wins_match else 0\n",
    "\n",
    "    return pd.Series([total_wins, total_noms, oscar_wins, oscar_noms, bafta_wins, bafta_noms],\n",
    "                     index=[\"total_wins\", \"total_noms\", \"oscar_wins\", \"oscar_noms\", \"bafta_wins\", \"bafta_noms\"])\n",
    "\n",
    "\n",
    "def transform_awards(X):\n",
    "    \"\"\"\n",
    "    Expects X to be a DataFrame with a single column (e.g., 'awards').\n",
    "    Applies extract_awards_info row-wise and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    # Apply the function to the first (and only) column\n",
    "    return X.iloc[:, 0].apply(extract_awards_info)\n",
    "\n",
    "# Wrap the function in a FunctionTransformer\n",
    "awards_transformer = FunctionTransformer(transform_awards, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-lable categorical features adjustment\n",
    "\n",
    "Below ``FunctionTransformer`` will group the given multi-lable feature into a top N + Others categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_top_categories(X, column, top_n, delimiter=\",\", others_label=\"Others\"):\n",
    "    \"\"\"\n",
    "    Transforms a multi-label column by keeping only the top_n categories (based on frequency)\n",
    "    and replacing all other categories with a generic label.\n",
    "    \n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Input DataFrame.\n",
    "        column (str): The name of the multi-label column to process.\n",
    "        top_n (int): Number of top categories to keep.\n",
    "        delimiter (str): Delimiter separating the values.\n",
    "        others_label (str): Label to assign to categories not among the top_n.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with one column (the processed column).\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    # Split the column values, explode, and count frequencies.\n",
    "    exploded = X[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    top_categories = counts.head(top_n).index.tolist()\n",
    "    \n",
    "    def map_categories(cell):\n",
    "        if pd.isna(cell):\n",
    "            return cell\n",
    "        # Split and strip each value.\n",
    "        cats = [cat.strip() for cat in cell.split(delimiter)]\n",
    "        # Replace values not in top_categories with others_label.\n",
    "        new_cats = [cat if cat in top_categories else others_label for cat in cats]\n",
    "        # Remove duplicates while preserving order.\n",
    "        seen = set()\n",
    "        new_cats = [x for x in new_cats if x not in seen and not seen.add(x)]\n",
    "        return delimiter.join(new_cats)\n",
    "    \n",
    "    X[column] = X[column].apply(map_categories)\n",
    "    # Return a DataFrame with just the transformed column.\n",
    "    return X[[column]]\n",
    "\n",
    "# Now, to create a FunctionTransformer for, say, the 'production_country_name' column with top_n=5:\n",
    "transformer_prod_country = FunctionTransformer(\n",
    "    func=partial(transform_top_categories, column=\"production_country_name\", top_n=5, delimiter=\",\", others_label=\"Others\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "# Similarly, for 'spoken_languages' column with top_n=5:\n",
    "transformer_spoken_lang = FunctionTransformer(\n",
    "    func=partial(transform_top_categories, column=\"spoken_languages\", top_n=5, delimiter=\",\", others_label=\"Others\"),\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release date rework\n",
    "\n",
    "`release_date` can be split int oseparate integer columns to be used during training later but binary features like `is_weekend` and `is_holiday` can also be derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "    df['release_year'] = df['release_date'].dt.year\n",
    "    df['release_month'] = df['release_date'].dt.month\n",
    "    df['release_day'] = df['release_date'].dt.day\n",
    "    df['is_weekend'] = (df['release_date'].dt.weekday >= 4).astype(int)\n",
    "    df['is_holiday_season'] = df['release_month'].isin([6, 7, 11, 12]).astype(int)\n",
    "    df['movie_age'] = 2025 - df['release_year']\n",
    "    return df\n",
    "\n",
    "# Wrap the function as a transformer\n",
    "date_features_transformer = FunctionTransformer(add_date_features, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return on Investment\n",
    "Great option for looking into the budget-revenue relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roi(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['roi'] = (df['revenue'] - df['budget']) / df['budget']\n",
    "    return df\n",
    "\n",
    "# Wrap the function as a transformer\n",
    "roi_transformer = FunctionTransformer(calculate_roi, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `actors`, `directors` and `writers`\n",
    "\n",
    "These features are quite valuable I think but they need to be reworked. I do know that IMDB has these ones ordered by importance and billing so we can get the most relevant people in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['first_billing_actor'] = df['actors'].apply(lambda x: x.split(',')[0].strip() if pd.notnull(x) and len(x.split(',')) > 0 else None)\n",
    "    df['second_billing_actor'] = df['actors'].apply(lambda x: x.split(',')[1].strip() if pd.notnull(x) and len(x.split(',')) > 1 else None)\n",
    "    df['third_billing_actor'] = df['actors'].apply(lambda x: x.split(',')[2].strip() if pd.notnull(x) and len(x.split(',')) > 2 else None)\n",
    "    df['forth_billing_actor'] = df['actors'].apply(lambda x: x.split(',')[3].strip() if pd.notnull(x) and len(x.split(',')) > 3 else None)\n",
    "    # df['fifth_billing_actor'] = df['actors'].apply(lambda x: x.split(',')[4].strip() if pd.notnull(x) and len(x.split(',')) > 4 else None)\n",
    "    return df.drop(columns=['actors'])\n",
    "\n",
    "def extract_directors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['main_director'] = df['director'].apply(lambda x: x.split(',')[0].strip() if pd.notnull(x) and len(x.split(',')) > 0 else None)\n",
    "    df['secondary_director'] = df['director'].apply(lambda x: x.split(',')[1].strip() if pd.notnull(x) and len(x.split(',')) > 1 else None)\n",
    "    return df.drop(columns=['director'])\n",
    "\n",
    "def extract_writers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['main_writer'] = df['writer'].apply(lambda x: x.split(',')[0].strip() if pd.notnull(x) and len(x.split(',')) > 0 else None)\n",
    "    df['secondary_writer'] = df['writer'].apply(lambda x: x.split(',')[1].strip() if pd.notnull(x) and len(x.split(',')) > 1 else None)\n",
    "    return df.drop(columns=['writer'])\n",
    "\n",
    "# Create FunctionTransformers\n",
    "actors_transformer = FunctionTransformer(extract_actors)\n",
    "directors_transformer = FunctionTransformer(extract_directors)\n",
    "writers_transformer = FunctionTransformer(extract_writers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropper\n",
    "Seetting up a dropper transformer that can be used in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function transformer to drop unwanted columns.\n",
    "def drop_unwanted_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    return df.drop(columns=columns, errors='ignore')\n",
    "\n",
    "columns_to_drop = ['production_company_name', 'director', 'writer', 'actors', 'title', 'release_date']\n",
    "\n",
    "dropper = FunctionTransformer(drop_unwanted_columns, kw_args={'columns': columns_to_drop})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main ColumnTransformer\n",
    "This `ColumnTransformer` will apply the included steps simultaniously. Each step relies on its own set of features so there is no overlap and no chance of issues. `Pipeline` applies the step in order which I will make use of later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('missing_indicator', missing_indicator_transformer, ['metascore', 'rotten_tomatoes_rating', 'meta_critic_rating', 'budget', 'revenue']),\n",
    "        ('awards', awards_transformer, ['awards']),\n",
    "        ('date_feature_engineering', date_features_transformer, ['release_date']),\n",
    "        ('top_n_prod_country', transformer_prod_country, ['production_country_name']),\n",
    "        ('top_n_spoken_lang', transformer_spoken_lang, ['spoken_languages']),\n",
    "        ('actors', actors_transformer, ['actors']),\n",
    "        ('directors', directors_transformer, ['director']),\n",
    "        ('writers', writers_transformer, ['writer']),\n",
    "        ('to_numeric', to_numeric, ['imdb_rating', 'imdb_votes'])\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "main_transformer.set_output(transform='pandas')\n",
    "\n",
    "# Apply the preprocessor to the data\n",
    "# clean_data = main_transformer.fit_transform(data)\n",
    "# clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = main_transformer.fit_transform(data)\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "#### Full data pipeline\n",
    "\n",
    "The main transformer will be applied so that all the feature transformations and engineering gets applied on actual data and then all the missing data will be imputed. I like this order because it makes sure the generated features are all based on real data. \n",
    "\n",
    "I would like to experiment a bit with various inputs of the imputation to study the effects on model performance and prediction. Let's start with running the imputation on all available data simultaniously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up pipeline\n",
    "imputation_pipeline = Pipeline(steps=[\n",
    "    # ('main_transformer', main_transformer),\n",
    "    ('impute_data', imputation_transformer),\n",
    "    # ('roi_feature_engineering', roi_transformer),\n",
    "    # ('dropper', dropper)\n",
    "])\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "imputation_pipeline.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_full = imputation_pipeline.fit_transform(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset using modern utilities\n",
    "# Parquet format: faster, smaller, preserves types\n",
    "save_processed_data(imputed_data_full, \"imputed_data_full\", format=\"parquet\")\n",
    "print(\"✅ Saved to data/processed/imputed_data_full.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Revenue Pipeline\n",
    "Let's exclude the target of my predictive modelimg which will be `revenue` to make sure the model cannot gain any information due to the reelations created by the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_no_revenue = imputation_pipeline.fit_transform(clean_data.drop(columns=['revenue', 'revenue_missing']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data_no_revenue['revenue'] = clean_data['revenue']\n",
    "imputed_data_no_revenue.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "save_processed_data(imputed_data_no_revenue, \"imputed_data_no_revenue\", format=\"parquet\")\n",
    "print(\"✅ Saved to data/processed/imputed_data_no_revenue.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data = clean_data[clean_data['revenue'] >= 50000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.dropna(subset=['revenue'])\n",
    "\n",
    "target = 'revenue'\n",
    "X = clean_data.drop(columns=[target, 'revenue_missing'], axis=1)\n",
    "y = clean_data[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pipeline = imputation_pipeline.fit(X, y)\n",
    "X_train_imputed = split_pipeline.transform(X_train)\n",
    "X_test_imputed = split_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/test splits as artifacts\n",
    "# These are model-specific outputs, so they go in artifacts/\n",
    "save_artifacts(X_train_imputed, \"X_train_imputed\", format=\"parquet\")\n",
    "save_artifacts(X_test_imputed, \"X_test_imputed\", format=\"parquet\")\n",
    "save_artifacts(y_train, \"y_train\", format=\"parquet\")\n",
    "save_artifacts(y_test, \"y_test\", format=\"parquet\")\n",
    "\n",
    "print(\"✅ Saved training artifacts to data/artifacts/\")\n",
    "print(f\"   - X_train: {len(X_train_imputed)} rows × {len(X_train_imputed.columns)} columns\")\n",
    "print(f\"   - X_test: {len(X_test_imputed)} rows × {len(X_test_imputed.columns)} columns\")\n",
    "print(f\"   - y_train: {len(y_train)} rows\")\n",
    "print(f\"   - y_test: {len(y_test)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Are-You-Not-Entertained",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, FunctionTransformer\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory config to project root to insure consistency across environments for project specific imports\n",
    "from pyprojroot import here\n",
    "os.chdir(here())\n",
    "\n",
    "# Project specific imports\n",
    "from src.utils import save_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quering and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB related imports\n",
    "from database.db_utils import init_db\n",
    "from config.config_loader import load_config\n",
    "from database.queries import prepped_data_query\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Initialize local PostgreSQL session\n",
    "Session = init_db(load_config(\"DB_URL\"))\n",
    "session = Session()\n",
    "\n",
    "# Execute and fetch results\n",
    "data = session.execute(text(prepped_data_query))\n",
    "\n",
    "# Close the session\n",
    "session.close()\n",
    "\n",
    "# Convert to DataFrame\n",
    "data = pd.DataFrame(data.fetchall(), columns=data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned dataset for future use\n",
    "save_dataframe(data, \"00_base_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"./data/00_base_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"missing_budget\"] = (data[\"budget\"] == 0).astype(int)\n",
    "# data[\"missing_revenue\"] = (data[\"revenue\"] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"budget\"] = data[\"budget\"].replace(0, np.nan)\n",
    "# data[\"revenue\"] = data[\"revenue\"].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[(data[\"budget\"] != 0) & (data[\"revenue\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[(data[\"budget\"] != 0) | (data[\"revenue\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilable Categorical Features\n",
    "\n",
    "There are a nuymber of multilable categorical features that will need to be looked into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_values_for_feature(df: pd.DataFrame, feature: str, delimiter: str = \",\") -> int:\n",
    "    \"\"\"\n",
    "    Splits the specified feature column by the delimiter and returns the number of unique values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        feature (str): The name of the column to process.\n",
    "        delimiter (str): The delimiter used to separate multiple values in the column.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of unique values.\n",
    "    \"\"\"\n",
    "    return len(df[feature].dropna().str.split(rf\"{delimiter}\\s*\").explode().unique())\n",
    "\n",
    "# List of features you want to analyze:\n",
    "features = [\n",
    "    \"genre_names\", \n",
    "    \"production_company_name\", \"production_country_name\", \n",
    "            \"spoken_languages\", \"director\", \"writer\", \"actors\"]\n",
    "\n",
    "# Create a dictionary with the counts for each feature:\n",
    "unique_counts = {feature: count_unique_values_for_feature(data, feature) for feature in features}\n",
    "\n",
    "# Display the results:\n",
    "for feature, count in unique_counts.items():\n",
    "    print(f\"{feature}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_categories(df: pd.DataFrame, column: str, top_n: int, delimiter: str = \",\", others_label: str = \"Others\") -> None:\n",
    "    \"\"\"\n",
    "    Prints the top_n unique values from a multi-label column and the total count of values \n",
    "    that fall outside the top_n (which would be grouped as 'Others').\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing your data.\n",
    "        column (str): The name of the multi-label column.\n",
    "        top_n (int): The number of top categories to display.\n",
    "        delimiter (str): The delimiter separating multiple values (default is a comma).\n",
    "        others_label (str): The label used for less frequent values.\n",
    "    \"\"\"\n",
    "    # Split the column into individual values and count frequencies\n",
    "    exploded = df[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    \n",
    "    # Get the top N categories and the sum for the rest\n",
    "    top_categories = counts.head(top_n)\n",
    "    others_count = counts[counts.index.difference(top_categories.index)].sum()\n",
    "    \n",
    "    print(\"--------------------------------------------------||\")\n",
    "    print(f\"Top {top_n} unique values for '{column}':\")\n",
    "    print(top_categories)\n",
    "    print(f\"Total count of all other values (will be grouped as '{others_label}'): {others_count}\")\n",
    "    print(\"--------------------------------------------------||\\n\")\n",
    "\n",
    "\n",
    "top_values = {\n",
    "    \"genre_names\": 20,\n",
    "    \"production_company_name\": 20,\n",
    "    \"production_country_name\": 10,\n",
    "    \"spoken_languages\": 10,\n",
    "    \"director\": 20,\n",
    "    \"writer\": 20,\n",
    "    \"actors\": 20\n",
    "}\n",
    "\n",
    "for feature, top_n in top_values.items():\n",
    "    print_top_categories(data, feature, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_top_categories(df: pd.DataFrame, column: str, top_n: int, delimiter: str = \",\", others_label: str = \"Others\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Processes a multi-label column by keeping only the top_n categories (based on frequency) \n",
    "    and replacing any other category with the 'others_label'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing your data.\n",
    "        column (str): The name of the multi-label column.\n",
    "        top_n (int): The number of top categories to keep.\n",
    "        delimiter (str): The delimiter separating multiple values in the column.\n",
    "        others_label (str): The label to use for all categories not in the top_n.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A new Series with the modified values.\n",
    "    \"\"\"\n",
    "    # Split the column and count frequencies\n",
    "    exploded = df[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    top_categories = counts.head(top_n).index.tolist()\n",
    "    \n",
    "    def map_categories(cell):\n",
    "        if pd.isna(cell):\n",
    "            return cell\n",
    "        cats = [cat.strip() for cat in cell.split(delimiter)]\n",
    "        # Replace any category not in top_categories with others_label\n",
    "        new_cats = [cat if cat in top_categories else others_label for cat in cats]\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        new_cats = [x for x in new_cats if x not in seen and not seen.add(x)]\n",
    "        return delimiter.join(new_cats)\n",
    "    \n",
    "    return df[column].apply(map_categories)\n",
    "\n",
    "top_values = {\n",
    "    # \"genre_names\": 20,  # Keeping all genres\n",
    "    # \"production_company_name\": 20,  # Too granular\n",
    "    \"production_country_name\": 5,\n",
    "    \"spoken_languages\": 5,\n",
    "    # \"director\": 20,  # Too granular\n",
    "    # \"writer\": 20,  # Too granular\n",
    "    # \"actors\": 20  # Too granular\n",
    "}\n",
    "\n",
    "# for feature, top_n in top_values.items():\n",
    "#     data[feature] = group_top_categories(data, feature, top_n)\n",
    "\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Nulls\n",
    "\n",
    "### Simple Imputer\n",
    "\n",
    "#### Categorical Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "num_cols = make_column_selector(dtype_include=['number'])\n",
    "num_cols = num_cols(data)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = make_column_selector(dtype_include=['object'])\n",
    "cat_cols = cat_cols(data)\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer cols\n",
    "cat_cols = ['production_company_name', 'production_country_name', 'spoken_languages', 'director', 'writer', 'actors', 'age_rating']\n",
    "zero_cols = ['imdb_rating', 'imdb_votes']\n",
    "median_cols = ['runtime_in_min']\n",
    "columns_to_drop = [\n",
    "    # 'title', \n",
    "    # 'awards',\n",
    "    'release_date'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iter columns setup\n",
    "iter_cols = [\n",
    "    'metascore', 'rotten_tomatoes_rating', 'meta_critic_rating',\n",
    "    'budget', 'revenue'\n",
    "    ]\n",
    "\n",
    "predictor_cols = [\n",
    "    'tmdb_vote_count', 'tmdb_vote_average', 'runtime_in_min', 'tmdb_popularity', 'imdb_rating', \n",
    "    'imdb_votes', 'total_wins', 'total_noms', 'oscar_wins', 'oscar_noms', 'bafta_wins', 'bafta_noms',\n",
    "    'metascore_missing', 'rotten_tomatoes_rating_missing', 'meta_critic_rating_missing',\n",
    "    ]\n",
    "\n",
    "full_iter_cols = iter_cols + predictor_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns imputer\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('cat_imputer', cat_imputer)\n",
    "    # ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# Numerical to zero columns imputer\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "zero_pipeline = Pipeline(steps=[\n",
    "    ('zero_imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# Numerical to median columns imputer\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "median_pipeline = Pipeline(steps=[\n",
    "    ('median_imputer', median_imputer)\n",
    "    # ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "# Iterative imputer\n",
    "iter_imputer = IterativeImputer(\n",
    "    estimator=BayesianRidge(),\n",
    "    # estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    initial_strategy='median',\n",
    "    n_nearest_features=None,\n",
    "    imputation_order='ascending',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "iter_pipeline = Pipeline(steps=[\n",
    "    ('cat_imputer', iter_imputer)\n",
    "    # ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add missing indicators for certain columns.\n",
    "def add_missing_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        df[col + \"_missing\"] = df[col].isnull().astype(int)\n",
    "    return df\n",
    "\n",
    "missing_indicator_transformer = FunctionTransformer(add_missing_indicators, validate=False)\n",
    "\n",
    "\n",
    "# Define a function transformer to drop unwanted columns.\n",
    "def drop_unwanted_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "dropper = FunctionTransformer(drop_unwanted_columns)\n",
    "\n",
    "\n",
    "# To numeric\n",
    "def convert_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in df.columns:\n",
    "        # Convert to string, remove commas, then convert to numeric\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "    return df\n",
    "\n",
    "to_numeric = FunctionTransformer(convert_to_numeric, validate=False)\n",
    "\n",
    "\n",
    "# def convert_to_numeric_cols(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "#     df = df.copy()\n",
    "#     for col in cols:\n",
    "#         # Remove commas and convert to numeric (coercing errors to NaN)\n",
    "#         df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "#     return df\n",
    "\n",
    "# to_numeric_transformer = FunctionTransformer(lambda X: convert_to_numeric_cols(X, cols=['imdb_votes']),\n",
    "#                                              validate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_awards_info(awards_str):\n",
    "    \"\"\"\n",
    "    Extracts numerical awards information from a text string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    awards_str : str\n",
    "        The awards description string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A Series with the following index:\n",
    "        [\"total_wins\", \"total_noms\", \"oscar_wins\", \"oscar_noms\", \"bafta_wins\", \"bafta_noms\"]\n",
    "    \"\"\"\n",
    "    # Handle missing or \"N/A\" values.\n",
    "    if pd.isna(awards_str) or awards_str.strip() in [\"N/A\", \"\"]:\n",
    "        return pd.Series([0, 0, 0, 0, 0, 0],\n",
    "                         index=[\"total_wins\", \"total_noms\", \"oscar_wins\", \"oscar_noms\", \"bafta_wins\", \"bafta_noms\"])\n",
    "    \n",
    "    # Extract overall totals.\n",
    "    # Look for a pattern like \"56 wins\" (we use negative lookahead to avoid picking up Oscar wins)\n",
    "    total_wins_match = re.search(r'(\\d+)\\s+wins?(?!.*Oscars)', awards_str, flags=re.IGNORECASE)\n",
    "    total_noms_match = re.search(r'(\\d+)\\s+nominations', awards_str, flags=re.IGNORECASE)\n",
    "    total_wins = int(total_wins_match.group(1)) if total_wins_match else 0\n",
    "    total_noms = int(total_noms_match.group(1)) if total_noms_match else 0\n",
    "\n",
    "    # Oscar-specific extraction:\n",
    "    oscar_noms_match = re.search(r'Nominated for\\s+(\\d+)\\s+Oscars?', awards_str, flags=re.IGNORECASE)\n",
    "    oscar_noms = int(oscar_noms_match.group(1)) if oscar_noms_match else 0\n",
    "    # Look for something like \"Oscars. 56 wins\" or \"Oscars 56 wins\" (using non-digit separator)\n",
    "    oscar_wins_match = re.search(r'Oscars?[\\W_]+(\\d+)\\s+wins?', awards_str, flags=re.IGNORECASE)\n",
    "    oscar_wins = int(oscar_wins_match.group(1)) if oscar_wins_match else 0\n",
    "\n",
    "    # BAFTA-specific extraction:\n",
    "    # For nominations, sometimes the text might run together (e.g. \"BAFTA Award28 nominations total\")\n",
    "    bafta_noms_match = re.search(r'Nominated for\\s+(\\d+)\\s*BAFTA', awards_str, flags=re.IGNORECASE)\n",
    "    bafta_noms = int(bafta_noms_match.group(1)) if bafta_noms_match else 0\n",
    "    # For wins, allow an optional \"Award\" word after BAFTA.\n",
    "    bafta_wins_match = re.search(r'BAFTA(?:\\s+Award)?[\\D_]+(\\d+)\\s+wins?', awards_str, flags=re.IGNORECASE)\n",
    "    bafta_wins = int(bafta_wins_match.group(1)) if bafta_wins_match else 0\n",
    "\n",
    "    return pd.Series([total_wins, total_noms, oscar_wins, oscar_noms, bafta_wins, bafta_noms],\n",
    "                     index=[\"total_wins\", \"total_noms\", \"oscar_wins\", \"oscar_noms\", \"bafta_wins\", \"bafta_noms\"])\n",
    "\n",
    "\n",
    "def transform_awards(X):\n",
    "    \"\"\"\n",
    "    Expects X to be a DataFrame with a single column (e.g., 'awards').\n",
    "    Applies extract_awards_info row-wise and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    # Apply the function to the first (and only) column\n",
    "    return X.iloc[:, 0].apply(extract_awards_info)\n",
    "\n",
    "# Wrap the function in a FunctionTransformer\n",
    "awards_transformer = FunctionTransformer(transform_awards, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def transform_top_categories(X, column, top_n, delimiter=\",\", others_label=\"Others\"):\n",
    "    \"\"\"\n",
    "    Transforms a multi-label column by keeping only the top_n categories (based on frequency)\n",
    "    and replacing all other categories with a generic label.\n",
    "    \n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Input DataFrame.\n",
    "        column (str): The name of the multi-label column to process.\n",
    "        top_n (int): Number of top categories to keep.\n",
    "        delimiter (str): Delimiter separating the values.\n",
    "        others_label (str): Label to assign to categories not among the top_n.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with one column (the processed column).\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    # Split the column values, explode, and count frequencies.\n",
    "    exploded = X[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    top_categories = counts.head(top_n).index.tolist()\n",
    "    \n",
    "    def map_categories(cell):\n",
    "        if pd.isna(cell):\n",
    "            return cell\n",
    "        # Split and strip each value.\n",
    "        cats = [cat.strip() for cat in cell.split(delimiter)]\n",
    "        # Replace values not in top_categories with others_label.\n",
    "        new_cats = [cat if cat in top_categories else others_label for cat in cats]\n",
    "        # Remove duplicates while preserving order.\n",
    "        seen = set()\n",
    "        new_cats = [x for x in new_cats if x not in seen and not seen.add(x)]\n",
    "        return delimiter.join(new_cats)\n",
    "    \n",
    "    X[column] = X[column].apply(map_categories)\n",
    "    # Return a DataFrame with just the transformed column.\n",
    "    return X[[column]]\n",
    "\n",
    "# Now, to create a FunctionTransformer for, say, the 'production_country_name' column with top_n=5:\n",
    "transformer_prod_country = FunctionTransformer(\n",
    "    func=partial(transform_top_categories, column=\"production_country_name\", top_n=5, delimiter=\",\", others_label=\"Others\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "# Similarly, for 'spoken_languages' column with top_n=5:\n",
    "transformer_spoken_lang = FunctionTransformer(\n",
    "    func=partial(transform_top_categories, column=\"spoken_languages\", top_n=5, delimiter=\",\", others_label=\"Others\"),\n",
    "    validate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "    df['release_year'] = df['release_date'].dt.year\n",
    "    df['release_month'] = df['release_date'].dt.month\n",
    "    df['release_day'] = df['release_date'].dt.day\n",
    "    df['is_weekend'] = (df['release_date'].dt.weekday >= 4).astype(int)\n",
    "    df['is_holiday_season'] = df['release_month'].isin([6, 7, 11, 12]).astype(int)\n",
    "    df['movie_age'] = 2025 - df['release_year']\n",
    "    return df\n",
    "\n",
    "# Wrap the function as a transformer\n",
    "date_features_transformer = FunctionTransformer(add_date_features, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roi(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['roi'] = (df['revenue'] - df['budget']) / df['budget']\n",
    "    return df\n",
    "\n",
    "# Wrap the function as a transformer\n",
    "roi_transformer = FunctionTransformer(calculate_roi, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('dropper', 'drop', columns_to_drop)\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "# Set output to pandas dataframe\n",
    "drop_transformer.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('to_numeric', to_numeric, zero_cols)\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "# Set output to pandas dataframe\n",
    "numeric_transformer.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('zero', zero_pipeline, zero_cols),\n",
    "        ('median', median_pipeline, median_cols),\n",
    "        ('cat', cat_pipeline, cat_cols),\n",
    "        ('awards', awards_transformer, ['awards']),\n",
    "        ('date_features', date_features_transformer, ['release_date']),\n",
    "        # ('roi', roi_transformer, ['budget', 'revenue']),\n",
    "        ('dropper', dropper, columns_to_drop)\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "main_transformer.set_output(transform='pandas')\n",
    "\n",
    "# Apply the preprocessor to the data\n",
    "# clean_data = main_transformer.fit_transform(data)\n",
    "# clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('prod_country', transformer_prod_country, ['production_country_name']),\n",
    "        ('spoken_lang', transformer_spoken_lang, ['spoken_languages'])\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "top_n_transformer.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_indicator_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('missing_indicator', missing_indicator_transformer, iter_cols),\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "missing_indicator_transformer.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # ('iter', iter_pipeline, iter_cols),\n",
    "        ('iter', iter_pipeline, full_iter_cols),\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "iter_transformer.set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline(steps=[\n",
    "    ('numeric_transformer', numeric_transformer),\n",
    "    ('main_transformer', main_transformer),\n",
    "    ('top_n_transformer', top_n_transformer),\n",
    "    ('missing_indicator_transformer', missing_indicator_transformer),\n",
    "    ('iter_preprocessor', iter_transformer),\n",
    "    ('roi', roi_transformer),\n",
    "    ('date_feature_engineering', date_features_transformer),\n",
    "    ('drop_transformer', drop_transformer)\n",
    "])\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "full_pipeline.set_output(transform='pandas')\n",
    "\n",
    "clean_data = full_pipeline.fit_transform(data)\n",
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data['release_date'] = pd.to_datetime(clean_data['release_date'])\n",
    "# clean_data['release_year'] = clean_data['release_date'].dt.year\n",
    "# clean_data['release_month'] = clean_data['release_date'].dt.month\n",
    "# clean_data['release_day'] = clean_data['release_date'].dt.day\n",
    "\n",
    "# clean_data['is_weekend'] = (clean_data['release_date'].dt.weekday >= 4).astype(int)  # 1 if Fri-Sun\n",
    "# clean_data['is_holiday_season'] = clean_data['release_month'].isin([6, 7, 11, 12]).astype(int)  # Summer & winter holidays\n",
    "# clean_data['movie_age'] = 2025 - clean_data['release_year']  # If predicting in 2025\n",
    "\n",
    "# clean_data.drop(columns=['release_date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_data['roi'] = (clean_data['revenue'] - clean_data['budget']) / clean_data['budget']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleanded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned dataset for future use\n",
    "save_dataframe(clean_data, \"01_clean_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

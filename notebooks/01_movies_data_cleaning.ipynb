{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory config to project root to insure consistency across environments for project specific imports\n",
    "from pyprojroot import here\n",
    "os.chdir(here())\n",
    "\n",
    "# Project specific imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quering and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB related imports\n",
    "# from src.utils import save_dataframe\n",
    "# from database.db_utils import init_db\n",
    "# from config.config_loader import load_config\n",
    "# from database.queries import prepped_data_query\n",
    "# from sqlalchemy import text\n",
    "\n",
    "# # Initialize local PostgreSQL session\n",
    "# Session = init_db(load_config(\"DB_URL\"))\n",
    "# session = Session()\n",
    "\n",
    "# # Execute and fetch results\n",
    "# data = session.execute(text(prepped_data_query))\n",
    "\n",
    "# # Close the session\n",
    "# session.close()\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# data = pd.DataFrame(data.fetchall(), columns=data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned dataset for future use\n",
    "# save_dataframe(data, \"00_base_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv(\"./data/00_base_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"missing_budget\"] = (data[\"budget\"] == 0).astype(int)\n",
    "# data[\"missing_revenue\"] = (data[\"revenue\"] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"budget\"] = data[\"budget\"].replace(0, np.nan)\n",
    "# data[\"revenue\"] = data[\"revenue\"].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[(data[\"budget\"] != 0) & (data[\"revenue\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[(data[\"budget\"] != 0) | (data[\"revenue\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3003 entries, 0 to 3002\n",
      "Data columns (total 22 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   title                    3003 non-null   object \n",
      " 1   release_date             3003 non-null   object \n",
      " 2   tmdb_vote_count          3003 non-null   int64  \n",
      " 3   tmdb_vote_average        3003 non-null   float64\n",
      " 4   genre_names              3003 non-null   object \n",
      " 5   budget                   1594 non-null   float64\n",
      " 6   revenue                  1816 non-null   float64\n",
      " 7   runtime_in_min           3001 non-null   float64\n",
      " 8   tmdb_popularity          3003 non-null   float64\n",
      " 9   production_company_name  2990 non-null   object \n",
      " 10  production_country_name  2998 non-null   object \n",
      " 11  spoken_languages         3001 non-null   object \n",
      " 12  director                 3001 non-null   object \n",
      " 13  writer                   2972 non-null   object \n",
      " 14  actors                   2997 non-null   object \n",
      " 15  imdb_rating              2995 non-null   float64\n",
      " 16  imdb_votes               2997 non-null   object \n",
      " 17  metascore                2222 non-null   float64\n",
      " 18  age_rating               2727 non-null   object \n",
      " 19  awards                   2439 non-null   object \n",
      " 20  rotten_tomatoes_rating   2536 non-null   float64\n",
      " 21  meta_critic_rating       2222 non-null   float64\n",
      "dtypes: float64(9), int64(1), object(12)\n",
      "memory usage: 516.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                         0\n",
       "release_date                  0\n",
       "tmdb_vote_count               0\n",
       "tmdb_vote_average             0\n",
       "genre_names                   0\n",
       "budget                     1409\n",
       "revenue                    1187\n",
       "runtime_in_min                2\n",
       "tmdb_popularity               0\n",
       "production_company_name      13\n",
       "production_country_name       5\n",
       "spoken_languages              2\n",
       "director                      2\n",
       "writer                       31\n",
       "actors                        6\n",
       "imdb_rating                   8\n",
       "imdb_votes                    6\n",
       "metascore                   781\n",
       "age_rating                  276\n",
       "awards                      564\n",
       "rotten_tomatoes_rating      467\n",
       "meta_critic_rating          781\n",
       "dtype: int64"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>tmdb_vote_count</th>\n",
       "      <th>tmdb_vote_average</th>\n",
       "      <th>genre_names</th>\n",
       "      <th>budget</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime_in_min</th>\n",
       "      <th>tmdb_popularity</th>\n",
       "      <th>production_company_name</th>\n",
       "      <th>...</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>actors</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>metascore</th>\n",
       "      <th>age_rating</th>\n",
       "      <th>awards</th>\n",
       "      <th>rotten_tomatoes_rating</th>\n",
       "      <th>meta_critic_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carter</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>378</td>\n",
       "      <td>6.000</td>\n",
       "      <td>Action, Thriller, Crime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.0</td>\n",
       "      <td>27.043</td>\n",
       "      <td>Apeitda</td>\n",
       "      <td>...</td>\n",
       "      <td>Jung Byung-gil</td>\n",
       "      <td>Jung Byung-gil, Byeong-sik Jung</td>\n",
       "      <td>Joo Won, Kim Bo-min, Lee Sung-jae</td>\n",
       "      <td>5.1</td>\n",
       "      <td>10,297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shadow in My Eye</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>494</td>\n",
       "      <td>7.829</td>\n",
       "      <td>War, Drama, History</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>16.008</td>\n",
       "      <td>Miso Film</td>\n",
       "      <td>...</td>\n",
       "      <td>Ole Bornedal</td>\n",
       "      <td>Ole Bornedal</td>\n",
       "      <td>Bertram Bisgaard Enevoldsen, Ester Birch, Ella...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>17,277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 wins &amp; 13 nominations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Noelle</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>673</td>\n",
       "      <td>6.595</td>\n",
       "      <td>Family, Comedy, Fantasy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.040</td>\n",
       "      <td>Walt Disney Pictures</td>\n",
       "      <td>...</td>\n",
       "      <td>Marc Lawrence</td>\n",
       "      <td>Marc Lawrence</td>\n",
       "      <td>Anna Kendrick, Shirley MacLaine, Bill Hader</td>\n",
       "      <td>6.3</td>\n",
       "      <td>28,616</td>\n",
       "      <td>48.0</td>\n",
       "      <td>G</td>\n",
       "      <td>1 win total</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Play</td>\n",
       "      <td>2019-09-04</td>\n",
       "      <td>349</td>\n",
       "      <td>7.278</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>5.214</td>\n",
       "      <td>Chapter 2, Moonshaker</td>\n",
       "      <td>...</td>\n",
       "      <td>Anthony Marciano</td>\n",
       "      <td>Max Boublil, Anthony Marciano</td>\n",
       "      <td>Max Boublil, Alice Isaaz, Malik Zidi</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1,755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 nomination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dave Chappelle: Sticks &amp; Stones</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>320</td>\n",
       "      <td>7.600</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>7.683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Stan Lathan</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>8.3</td>\n",
       "      <td>28,457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>Won 3 Primetime Emmys. 4 wins &amp; 5 nominations ...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title release_date  tmdb_vote_count  \\\n",
       "0                           Carter   2022-08-05              378   \n",
       "1             The Shadow in My Eye   2021-10-28              494   \n",
       "2                           Noelle   2019-11-12              673   \n",
       "3                             Play   2019-09-04              349   \n",
       "4  Dave Chappelle: Sticks & Stones   2019-08-26              320   \n",
       "\n",
       "   tmdb_vote_average              genre_names  budget  revenue  \\\n",
       "0              6.000  Action, Thriller, Crime     NaN      NaN   \n",
       "1              7.829      War, Drama, History     NaN      NaN   \n",
       "2              6.595  Family, Comedy, Fantasy     NaN      NaN   \n",
       "3              7.278                   Comedy     NaN      NaN   \n",
       "4              7.600                   Comedy     NaN      NaN   \n",
       "\n",
       "   runtime_in_min  tmdb_popularity production_company_name  ...  \\\n",
       "0           132.0           27.043                 Apeitda  ...   \n",
       "1           107.0           16.008               Miso Film  ...   \n",
       "2           100.0           10.040    Walt Disney Pictures  ...   \n",
       "3           108.0            5.214   Chapter 2, Moonshaker  ...   \n",
       "4            65.0            7.683                     NaN  ...   \n",
       "\n",
       "           director                           writer  \\\n",
       "0    Jung Byung-gil  Jung Byung-gil, Byeong-sik Jung   \n",
       "1      Ole Bornedal                     Ole Bornedal   \n",
       "2     Marc Lawrence                    Marc Lawrence   \n",
       "3  Anthony Marciano    Max Boublil, Anthony Marciano   \n",
       "4       Stan Lathan                   Dave Chappelle   \n",
       "\n",
       "                                              actors imdb_rating imdb_votes  \\\n",
       "0                  Joo Won, Kim Bo-min, Lee Sung-jae         5.1     10,297   \n",
       "1  Bertram Bisgaard Enevoldsen, Ester Birch, Ella...         7.3     17,277   \n",
       "2        Anna Kendrick, Shirley MacLaine, Bill Hader         6.3     28,616   \n",
       "3               Max Boublil, Alice Isaaz, Malik Zidi         7.2      1,755   \n",
       "4                                     Dave Chappelle         8.3     28,457   \n",
       "\n",
       "   metascore age_rating                                             awards  \\\n",
       "0        NaN      TV-MA                                                NaN   \n",
       "1        NaN      TV-MA                            2 wins & 13 nominations   \n",
       "2       48.0          G                                        1 win total   \n",
       "3        NaN        NaN                                       1 nomination   \n",
       "4        NaN      TV-MA  Won 3 Primetime Emmys. 4 wins & 5 nominations ...   \n",
       "\n",
       "  rotten_tomatoes_rating meta_critic_rating  \n",
       "0                   32.0                NaN  \n",
       "1                    NaN                NaN  \n",
       "2                   56.0               48.0  \n",
       "3                    NaN                NaN  \n",
       "4                   42.0                NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilable Categorical Features\n",
    "\n",
    "There are a nuymber of multilable categorical features that will need to be looked into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre_names: 19 unique values\n",
      "production_company_name: 4358 unique values\n",
      "production_country_name: 83 unique values\n",
      "spoken_languages: 89 unique values\n",
      "director: 2251 unique values\n",
      "writer: 4593 unique values\n",
      "actors: 4961 unique values\n"
     ]
    }
   ],
   "source": [
    "def count_unique_values_for_feature(df: pd.DataFrame, feature: str, delimiter: str = \",\") -> int:\n",
    "    \"\"\"\n",
    "    Splits the specified feature column by the delimiter and returns the number of unique values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        feature (str): The name of the column to process.\n",
    "        delimiter (str): The delimiter used to separate multiple values in the column.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of unique values.\n",
    "    \"\"\"\n",
    "    return len(df[feature].dropna().str.split(rf\"{delimiter}\\s*\").explode().unique())\n",
    "\n",
    "# List of features you want to analyze:\n",
    "features = [\n",
    "    \"genre_names\", \n",
    "    \"production_company_name\", \"production_country_name\", \n",
    "            \"spoken_languages\", \"director\", \"writer\", \"actors\"]\n",
    "\n",
    "# Create a dictionary with the counts for each feature:\n",
    "unique_counts = {feature: count_unique_values_for_feature(data, feature) for feature in features}\n",
    "\n",
    "# Display the results:\n",
    "for feature, count in unique_counts.items():\n",
    "    print(f\"{feature}: {count} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------||"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 unique values for 'genre_names':\n",
      "genre_names\n",
      "Drama              1292\n",
      "Comedy              976\n",
      "Thriller            864\n",
      "Action              735\n",
      "Horror              514\n",
      "Adventure           487\n",
      "Romance             401\n",
      "Crime               393\n",
      "Science Fiction     388\n",
      "Fantasy             371\n",
      "Family              301\n",
      "Mystery             300\n",
      "Animation           294\n",
      "History             189\n",
      "War                  92\n",
      "Music                89\n",
      "Documentary          59\n",
      "TV Movie             29\n",
      "Western              27\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 0\n",
      "--------------------------------------------------||\n",
      "\n",
      "--------------------------------------------------||\n",
      "Top 20 unique values for 'production_company_name':\n",
      "production_company_name\n",
      "Universal Pictures          109\n",
      "Lionsgate                    91\n",
      "Columbia Pictures            81\n",
      "Paramount Pictures           77\n",
      "Warner Bros. Pictures        76\n",
      "Walt Disney Pictures         63\n",
      "Canal+                       61\n",
      "StudioCanal                  57\n",
      "France 2 Cinéma              51\n",
      "Blumhouse Productions        51\n",
      "Ciné+                        48\n",
      "Ingenious Media              47\n",
      "Bron Studios                 46\n",
      "TSG Entertainment            46\n",
      "FilmNation Entertainment     43\n",
      "New Line Cinema              43\n",
      "Film4 Productions            42\n",
      "STXfilms                     42\n",
      "20th Century Fox             42\n",
      "Pathé                        40\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 10262\n",
      "--------------------------------------------------||\n",
      "\n",
      "--------------------------------------------------||\n",
      "Top 10 unique values for 'production_country_name':\n",
      "production_country_name\n",
      "United States of America    2078\n",
      "United Kingdom               484\n",
      "France                       414\n",
      "Canada                       249\n",
      "Belgium                      139\n",
      "Italy                        122\n",
      "Germany                      119\n",
      "Spain                        117\n",
      "Japan                        113\n",
      "China                        110\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 801\n",
      "--------------------------------------------------||\n",
      "\n",
      "--------------------------------------------------||\n",
      "Top 10 unique values for 'spoken_languages':\n",
      "spoken_languages\n",
      "English       2424\n",
      "French         415\n",
      "Spanish        361\n",
      "Italian        193\n",
      "German         155\n",
      "Japanese       130\n",
      "Russian        103\n",
      "Portuguese      87\n",
      "Mandarin        85\n",
      "Korean          70\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 534\n",
      "--------------------------------------------------||\n",
      "\n",
      "--------------------------------------------------||\n",
      "Top 20 unique values for 'director':\n",
      "director\n",
      "Sam Liu               12\n",
      "Guy Ritchie            7\n",
      "David Leitch           7\n",
      "Taika Waititi          7\n",
      "Wes Anderson           7\n",
      "Tyler Perry            6\n",
      "Antoine Fuqua          6\n",
      "Ridley Scott           6\n",
      "Clint Eastwood         6\n",
      "Zack Snyder            6\n",
      "Quentin Dupieux        6\n",
      "Steven Soderbergh      6\n",
      "Paolo Sorrentino       5\n",
      "Mike Flanagan          5\n",
      "Doug Liman             5\n",
      "M. Night Shyamalan     5\n",
      "Jaume Collet-Serra     5\n",
      "David Gordon Green     5\n",
      "McG                    5\n",
      "Woody Allen            5\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 3196\n",
      "--------------------------------------------------||\n",
      "\n",
      "--------------------------------------------------||\n",
      "Top 20 unique values for 'writer':\n",
      "writer\n",
      "Stan Lee              13\n",
      "Stephen King          11\n",
      "Bob Kane              10\n",
      "Michael Green          9\n",
      "Rhett Reese            9\n",
      "Paul Wernick           9\n",
      "Peter Craig            8\n",
      "Wes Anderson           7\n",
      "Jack Thorne            7\n",
      "Shay Hatten            7\n",
      "Nicola Guaglianone     7\n",
      "James Wan              7\n",
      "Roald Dahl             7\n",
      "Bryan Woods            7\n",
      "Scott Beck             6\n",
      "Jérémie Guez           6\n",
      "Mike White             6\n",
      "Matt Lieberman         6\n",
      "Chris Sparling         6\n",
      "Richard Wenk           6\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 6255\n",
      "--------------------------------------------------||\n",
      "\n",
      "--------------------------------------------------||\n",
      "Top 20 unique values for 'actors':\n",
      "actors\n",
      "Nicolas Cage         25\n",
      "Samuel L. Jackson    17\n",
      "Ryan Reynolds        17\n",
      "Dwayne Johnson       16\n",
      "Mark Wahlberg        16\n",
      "Dave Bautista        16\n",
      "Tom Hanks            16\n",
      "Nicole Kidman        15\n",
      "Kevin Hart           15\n",
      "Woody Harrelson      15\n",
      "Elle Fanning         14\n",
      "Adam Driver          14\n",
      "Anna Kendrick        14\n",
      "Chris Hemsworth      13\n",
      "Gerard Butler        13\n",
      "Bruce Willis         13\n",
      "Jake Gyllenhaal      13\n",
      "Ben Affleck          13\n",
      "Adam Sandler         13\n",
      "Margot Robbie        13\n",
      "Name: count, dtype: int64\n",
      "Total count of all other values (will be grouped as 'Others'): 8688\n",
      "--------------------------------------------------||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_categories(df: pd.DataFrame, column: str, top_n: int, delimiter: str = \",\", others_label: str = \"Others\") -> None:\n",
    "    \"\"\"\n",
    "    Prints the top_n unique values from a multi-label column and the total count of values \n",
    "    that fall outside the top_n (which would be grouped as 'Others').\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing your data.\n",
    "        column (str): The name of the multi-label column.\n",
    "        top_n (int): The number of top categories to display.\n",
    "        delimiter (str): The delimiter separating multiple values (default is a comma).\n",
    "        others_label (str): The label used for less frequent values.\n",
    "    \"\"\"\n",
    "    # Split the column into individual values and count frequencies\n",
    "    exploded = df[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    \n",
    "    # Get the top N categories and the sum for the rest\n",
    "    top_categories = counts.head(top_n)\n",
    "    others_count = counts[counts.index.difference(top_categories.index)].sum()\n",
    "    \n",
    "    print(\"--------------------------------------------------||\")\n",
    "    print(f\"Top {top_n} unique values for '{column}':\")\n",
    "    print(top_categories)\n",
    "    print(f\"Total count of all other values (will be grouped as '{others_label}'): {others_count}\")\n",
    "    print(\"--------------------------------------------------||\\n\")\n",
    "\n",
    "\n",
    "top_values = {\n",
    "    \"genre_names\": 20,\n",
    "    \"production_company_name\": 20,\n",
    "    \"production_country_name\": 10,\n",
    "    \"spoken_languages\": 10,\n",
    "    \"director\": 20,\n",
    "    \"writer\": 20,\n",
    "    \"actors\": 20\n",
    "}\n",
    "\n",
    "for feature, top_n in top_values.items():\n",
    "    print_top_categories(data, feature, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_top_categories(df: pd.DataFrame, column: str, top_n: int, delimiter: str = \",\", others_label: str = \"Others\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Processes a multi-label column by keeping only the top_n categories (based on frequency) \n",
    "    and replacing any other category with the 'others_label'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing your data.\n",
    "        column (str): The name of the multi-label column.\n",
    "        top_n (int): The number of top categories to keep.\n",
    "        delimiter (str): The delimiter separating multiple values in the column.\n",
    "        others_label (str): The label to use for all categories not in the top_n.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A new Series with the modified values.\n",
    "    \"\"\"\n",
    "    # Split the column and count frequencies\n",
    "    exploded = df[column].dropna().str.split(rf\"{delimiter}\\s*\").explode().str.strip()\n",
    "    counts = exploded.value_counts()\n",
    "    top_categories = counts.head(top_n).index.tolist()\n",
    "    \n",
    "    def map_categories(cell):\n",
    "        if pd.isna(cell):\n",
    "            return cell\n",
    "        cats = [cat.strip() for cat in cell.split(delimiter)]\n",
    "        # Replace any category not in top_categories with others_label\n",
    "        new_cats = [cat if cat in top_categories else others_label for cat in cats]\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        new_cats = [x for x in new_cats if x not in seen and not seen.add(x)]\n",
    "        return delimiter.join(new_cats)\n",
    "    \n",
    "    return df[column].apply(map_categories)\n",
    "\n",
    "top_values = {\n",
    "    # \"genre_names\": 20,  # Keeping all genres\n",
    "    # \"production_company_name\": 20,  # Too granular\n",
    "    \"production_country_name\": 5,\n",
    "    \"spoken_languages\": 5,\n",
    "    # \"director\": 20,  # Too granular\n",
    "    # \"writer\": 20,  # Too granular\n",
    "    # \"actors\": 20  # Too granular\n",
    "}\n",
    "\n",
    "for feature, top_n in top_values.items():\n",
    "    data[feature] = group_top_categories(data, feature, top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>tmdb_vote_count</th>\n",
       "      <th>tmdb_vote_average</th>\n",
       "      <th>genre_names</th>\n",
       "      <th>budget</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime_in_min</th>\n",
       "      <th>tmdb_popularity</th>\n",
       "      <th>production_company_name</th>\n",
       "      <th>...</th>\n",
       "      <th>director</th>\n",
       "      <th>writer</th>\n",
       "      <th>actors</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>imdb_votes</th>\n",
       "      <th>metascore</th>\n",
       "      <th>age_rating</th>\n",
       "      <th>awards</th>\n",
       "      <th>rotten_tomatoes_rating</th>\n",
       "      <th>meta_critic_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carter</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>378</td>\n",
       "      <td>6.000</td>\n",
       "      <td>Action, Thriller, Crime</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.0</td>\n",
       "      <td>27.043</td>\n",
       "      <td>Apeitda</td>\n",
       "      <td>...</td>\n",
       "      <td>Jung Byung-gil</td>\n",
       "      <td>Jung Byung-gil, Byeong-sik Jung</td>\n",
       "      <td>Joo Won, Kim Bo-min, Lee Sung-jae</td>\n",
       "      <td>5.1</td>\n",
       "      <td>10,297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shadow in My Eye</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>494</td>\n",
       "      <td>7.829</td>\n",
       "      <td>War, Drama, History</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>16.008</td>\n",
       "      <td>Miso Film</td>\n",
       "      <td>...</td>\n",
       "      <td>Ole Bornedal</td>\n",
       "      <td>Ole Bornedal</td>\n",
       "      <td>Bertram Bisgaard Enevoldsen, Ester Birch, Ella...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>17,277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 wins &amp; 13 nominations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Noelle</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>673</td>\n",
       "      <td>6.595</td>\n",
       "      <td>Family, Comedy, Fantasy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.040</td>\n",
       "      <td>Walt Disney Pictures</td>\n",
       "      <td>...</td>\n",
       "      <td>Marc Lawrence</td>\n",
       "      <td>Marc Lawrence</td>\n",
       "      <td>Anna Kendrick, Shirley MacLaine, Bill Hader</td>\n",
       "      <td>6.3</td>\n",
       "      <td>28,616</td>\n",
       "      <td>48.0</td>\n",
       "      <td>G</td>\n",
       "      <td>1 win total</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Play</td>\n",
       "      <td>2019-09-04</td>\n",
       "      <td>349</td>\n",
       "      <td>7.278</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>5.214</td>\n",
       "      <td>Chapter 2, Moonshaker</td>\n",
       "      <td>...</td>\n",
       "      <td>Anthony Marciano</td>\n",
       "      <td>Max Boublil, Anthony Marciano</td>\n",
       "      <td>Max Boublil, Alice Isaaz, Malik Zidi</td>\n",
       "      <td>7.2</td>\n",
       "      <td>1,755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 nomination</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dave Chappelle: Sticks &amp; Stones</td>\n",
       "      <td>2019-08-26</td>\n",
       "      <td>320</td>\n",
       "      <td>7.600</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>7.683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Stan Lathan</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>Dave Chappelle</td>\n",
       "      <td>8.3</td>\n",
       "      <td>28,457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>Won 3 Primetime Emmys. 4 wins &amp; 5 nominations ...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title release_date  tmdb_vote_count  \\\n",
       "0                           Carter   2022-08-05              378   \n",
       "1             The Shadow in My Eye   2021-10-28              494   \n",
       "2                           Noelle   2019-11-12              673   \n",
       "3                             Play   2019-09-04              349   \n",
       "4  Dave Chappelle: Sticks & Stones   2019-08-26              320   \n",
       "\n",
       "   tmdb_vote_average              genre_names  budget  revenue  \\\n",
       "0              6.000  Action, Thriller, Crime     NaN      NaN   \n",
       "1              7.829      War, Drama, History     NaN      NaN   \n",
       "2              6.595  Family, Comedy, Fantasy     NaN      NaN   \n",
       "3              7.278                   Comedy     NaN      NaN   \n",
       "4              7.600                   Comedy     NaN      NaN   \n",
       "\n",
       "   runtime_in_min  tmdb_popularity production_company_name  ...  \\\n",
       "0           132.0           27.043                 Apeitda  ...   \n",
       "1           107.0           16.008               Miso Film  ...   \n",
       "2           100.0           10.040    Walt Disney Pictures  ...   \n",
       "3           108.0            5.214   Chapter 2, Moonshaker  ...   \n",
       "4            65.0            7.683                     NaN  ...   \n",
       "\n",
       "           director                           writer  \\\n",
       "0    Jung Byung-gil  Jung Byung-gil, Byeong-sik Jung   \n",
       "1      Ole Bornedal                     Ole Bornedal   \n",
       "2     Marc Lawrence                    Marc Lawrence   \n",
       "3  Anthony Marciano    Max Boublil, Anthony Marciano   \n",
       "4       Stan Lathan                   Dave Chappelle   \n",
       "\n",
       "                                              actors imdb_rating imdb_votes  \\\n",
       "0                  Joo Won, Kim Bo-min, Lee Sung-jae         5.1     10,297   \n",
       "1  Bertram Bisgaard Enevoldsen, Ester Birch, Ella...         7.3     17,277   \n",
       "2        Anna Kendrick, Shirley MacLaine, Bill Hader         6.3     28,616   \n",
       "3               Max Boublil, Alice Isaaz, Malik Zidi         7.2      1,755   \n",
       "4                                     Dave Chappelle         8.3     28,457   \n",
       "\n",
       "   metascore age_rating                                             awards  \\\n",
       "0        NaN      TV-MA                                                NaN   \n",
       "1        NaN      TV-MA                            2 wins & 13 nominations   \n",
       "2       48.0          G                                        1 win total   \n",
       "3        NaN        NaN                                       1 nomination   \n",
       "4        NaN      TV-MA  Won 3 Primetime Emmys. 4 wins & 5 nominations ...   \n",
       "\n",
       "  rotten_tomatoes_rating meta_critic_rating  \n",
       "0                   32.0                NaN  \n",
       "1                    NaN                NaN  \n",
       "2                   56.0               48.0  \n",
       "3                    NaN                NaN  \n",
       "4                   42.0                NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Nulls\n",
    "\n",
    "### Simple Imputer\n",
    "\n",
    "#### Categorical Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Create a transformer for categorical features\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")\n",
    "\n",
    "# List of your categorical columns where you want to replace nulls with \"Unknown\"\n",
    "cat_cols = ['production_company_name', 'production_country_name', 'spoken_languages', 'director', 'writer', 'actors', 'age_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply imputer\n",
    "# data[cat_cols] = cat_imputer.fit_transform(data[cat_cols])\n",
    "# data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "zero_cols = ['imdb_rating', 'imdb_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputer = SimpleImputer(strategy='median')\n",
    "median_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Initialize the IterativeImputer (using BayesianRidge as the default estimator)\n",
    "iter_imputer = IterativeImputer(\n",
    "    # estimator=BayesianRidge(),\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    initial_strategy='median',\n",
    "    n_nearest_features=None,\n",
    "    imputation_order='ascending',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "iter_cols = ['metascore', 'rotten_tomatoes_rating', 'meta_critic_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create missing indicator columns for each score column\n",
    "for col in iter_cols:\n",
    "    data[col + \"_missing\"] = data[col].isnull().astype(int)\n",
    "\n",
    "# data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "iter_impute = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('iter_impute', iter_imputer, iter_cols)\n",
    "    ],\n",
    "    remainder=\"passthrough\"  # Unlisted columns are kept unchanged (if any)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_pipeline, numeric_features),\n",
    "#         ('cat', categorical_pipeline, categorical_features),\n",
    "#         ('drop_cols', 'drop', ['id', 'unnecessary_column'])\n",
    "#     ],\n",
    "#     remainder=\"passthrough\"  # Unlisted columns are kept unchanged (if any)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, build a ColumnTransformer that imputes the groups that do not use IterativeImputer.\n",
    "process_nulls = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', cat_imputer, cat_cols),\n",
    "        ('zero', zero_imputer, zero_cols),\n",
    "        # ('median', median_imputer, median_cols),\n",
    "        # ('drop_cols', 'drop', columns_to_drop)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep other features (including those for iterative imputer) as is.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Then, create a pipeline that applies the preprocessor and then the iterative imputer to the remaining columns.\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocess', process_nulls),\n",
    "    ('iter_impute', iter_impute)\n",
    "    # ...followed by scaling and model training as needed.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Passing extra keyword arguments to ColumnTransformer.transform is only supported if enable_metadata_routing=True, which you can set using `sklearn.set_config`. See the User Guide <https://scikit-learn.org/stable/metadata_routing.html> for more details. Extra parameters passed are: {'copy'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[201], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m process_nulls\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# And transform the data:\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_nulls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1059\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[1;34m(self, X, **params)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform X separately by each transformer, concatenate results.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;124;03m        sparse matrices.\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1059\u001b[0m     \u001b[43m_raise_for_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m     X \u001b[38;5;241m=\u001b[39m _check_X(X)\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\utils\\_metadata_requests.py:158\u001b[0m, in \u001b[0;36m_raise_for_params\u001b[1;34m(params, owner, method)\u001b[0m\n\u001b[0;32m    154\u001b[0m caller \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mowner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;28;01melse\u001b[39;00m owner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled() \u001b[38;5;129;01mand\u001b[39;00m params:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing extra keyword arguments to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is only supported if\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m enable_metadata_routing=True, which you can set using\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `sklearn.set_config`. See the User Guide\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m <https://scikit-learn.org/stable/metadata_routing.html> for more\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m details. Extra parameters passed are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(params)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Passing extra keyword arguments to ColumnTransformer.transform is only supported if enable_metadata_routing=True, which you can set using `sklearn.set_config`. See the User Guide <https://scikit-learn.org/stable/metadata_routing.html> for more details. Extra parameters passed are: {'copy'}"
     ]
    }
   ],
   "source": [
    "# Now, you can fit the pipeline on your training data:\n",
    "pipeline.fit(data)\n",
    "# And transform the data:\n",
    "data = pipeline.transform(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'remainder__metascore'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:364\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[1;32m--> 364\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m \u001b[43mall_columns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col_idx, numbers\u001b[38;5;241m.\u001b[39mIntegral):\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'remainder__metascore'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 97\u001b[0m\n\u001b[0;32m     88\u001b[0m transformed_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m new_columns\n\u001b[0;32m     90\u001b[0m iter_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# ('preprocessor', preprocessor),\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# ('dropper', dropper),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Optionally, if you need additional encoding.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m ])\n\u001b[1;32m---> 97\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m \u001b[43miter_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m transformed_data\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# If you need the result back as a DataFrame, you can do:\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# (Here, we assume that the preprocessor returns a DataFrame; if it returns an array, you'll have to set column names accordingly.)\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# transformed_df = pd.DataFrame(transformed_data)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# transformed_df.head()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:730\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    724\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    725\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    726\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    727\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    728\u001b[0m )\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m    735\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    736\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:993\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[0;32m    991\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _routing_enabled():\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:552\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    550\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    551\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 552\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32mc:\\Users\\bokr4002\\OneDrive - NIQ\\Desktop\\GitReposLocal\\Are-You-Not-Entertained\\venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:372\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    369\u001b[0m         column_indices\u001b[38;5;241m.\u001b[39mappend(col_idx)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA given column is not a column of the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n",
      "\u001b[1;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# from sklearn.preprocessing import StandardScaler  # Uncomment if needed later\n",
    "# from sklearn.preprocessing import OneHotEncoder   # Uncomment if needed later\n",
    "\n",
    "# Define your column groups\n",
    "# zero_cols = ['some_numeric_col1', 'some_numeric_col2']  # Impute missing values with 0\n",
    "# median_cols = ['budget', 'revenue', 'runtime_in_min']    # Impute missing values with the median\n",
    "# cat_cols = ['production_company_name', 'production_country_name', 'spoken_languages', \n",
    "#             'director', 'writer', 'actors']              # Impute missing categorical values with \"Unknown\"\n",
    "            \n",
    "# Columns you want to drop after imputation (for example, identifiers or irrelevant features)\n",
    "# drop_cols = ['unnecessary_column1', 'unnecessary_column2']\n",
    "\n",
    "# Create individual pipelines for each group:\n",
    "\n",
    "zero_pipeline = Pipeline(steps=[\n",
    "    ('zero_imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "    # ('scaler', StandardScaler())  # Uncomment to scale these columns\n",
    "])\n",
    "\n",
    "median_pipeline = Pipeline(steps=[\n",
    "    ('median_imputer', SimpleImputer(strategy='median'))\n",
    "    # ('scaler', StandardScaler())  # Uncomment to scale these columns\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='Unknown'))\n",
    "    # ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Uncomment if you want one-hot encoding here\n",
    "])\n",
    "\n",
    "# impute_pipeline = Pipeline(steps=[\n",
    "#     ('iter_impute', iter_imputer),\n",
    "# ])\n",
    "\n",
    "# Combine them in a ColumnTransformer.\n",
    "# Any columns not mentioned in the transformers will be left unchanged due to remainder='passthrough'.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('zero', zero_pipeline, zero_cols),\n",
    "        ('median', median_pipeline, median_cols),\n",
    "        ('cat', cat_pipeline, cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "preprocessor.set_output(transform='pandas')\n",
    "\n",
    "# Define a function transformer to drop unwanted columns.\n",
    "def drop_unwanted_columns(X):\n",
    "    # Assuming X is a pandas DataFrame; errors='ignore' ensures that missing columns are skipped.\n",
    "    return X.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "dropper = FunctionTransformer(drop_unwanted_columns)\n",
    "\n",
    "\n",
    "iter_cols = ['remainder__metascore', 'remainder__rotten_tomatoes_rating', 'remainder__meta_critic_rating']\n",
    "\n",
    "# IterativeImputer applied only to selected columns\n",
    "iter_imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('iter', iter_imputer, iter_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Now, build the full pipeline.\n",
    "# Note: The IterativeImputer should be applied after the initial imputations, so it comes after the ColumnTransformer and dropper.\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dropper', dropper),\n",
    "    # ('iter_imputer', iter_imputer),\n",
    "    # ('scaler', StandardScaler()),  # Optionally, scale after imputation.\n",
    "    # ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Optionally, if you need additional encoding.\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "# Assume 'data' is your DataFrame with the full set of features.\n",
    "# You can then fit_transform the pipeline on your training data:\n",
    "transformed_data = full_pipeline.fit_transform(data)\n",
    "\n",
    "# Column adjustment\n",
    "# new_columns = [col.split(\"__\", 1)[-1] for col in transformed_data.columns]\n",
    "# transformed_data.columns = new_columns\n",
    "transformed_data.head()\n",
    "\n",
    "# If you need the result back as a DataFrame, you can do:\n",
    "# (Here, we assume that the preprocessor returns a DataFrame; if it returns an array, you'll have to set column names accordingly.)\n",
    "# transformed_df = pd.DataFrame(transformed_data)\n",
    "# transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   feature1  feature2  feature3\n",
      "0       1.0       NaN      10.0\n",
      "1       2.0       5.0       NaN\n",
      "2       NaN       6.0       NaN\n",
      "3       4.0       7.0      13.0\n",
      "\n",
      "DataFrame after Iterative Imputation:\n",
      "   feature1  feature2  feature3\n",
      "0      1.00      5.75     10.00\n",
      "1      2.00      5.00     10.66\n",
      "2      1.57      6.00     10.66\n",
      "3      4.00      7.00     13.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Sample data with missing values\n",
    "data = {\n",
    "    'feature1': [1.0, 2.0, None, 4.0],\n",
    "    'feature2': [None, 5.0, 6.0, 7.0],\n",
    "    'feature3': [10.0, None, None, 13.0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Initialize the IterativeImputer (using BayesianRidge as the default estimator)\n",
    "imputer = IterativeImputer(\n",
    "    # estimator=BayesianRidge(),\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    initial_strategy='median',\n",
    "    n_nearest_features=None,\n",
    "    imputation_order='ascending',\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "# Fit the imputer and transform the data\n",
    "imputed_array = imputer.fit_transform(df)\n",
    "imputed_df = pd.DataFrame(imputed_array, columns=df.columns)\n",
    "\n",
    "print(\"\\nDataFrame after Iterative Imputation:\")\n",
    "print(imputed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler  # Optional\n",
    "\n",
    "# Define lists of columns for each imputation strategy:\n",
    "iterative_cols = ['feature1', 'feature2']\n",
    "median_cols = ['feature3', 'feature4']\n",
    "zero_cols = ['feature5', 'feature6']\n",
    "\n",
    "# Create the imputer transformers:\n",
    "iterative_imputer = IterativeImputer(random_state=42)\n",
    "median_imputer = SimpleImputer(strategy='median')\n",
    "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# Optionally, you can also scale the numeric features after imputation.\n",
    "# For example, define a numeric transformer pipeline that applies imputation then scaling:\n",
    "iterative_pipeline = Pipeline(steps=[\n",
    "    ('imputer', iterative_imputer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "median_pipeline = Pipeline(steps=[\n",
    "    ('imputer', median_imputer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "zero_pipeline = Pipeline(steps=[\n",
    "    ('imputer', zero_imputer),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine all these using a ColumnTransformer:\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('iterative', iterative_pipeline, iterative_cols),\n",
    "        ('median', median_pipeline, median_cols),\n",
    "        ('zero', zero_pipeline, zero_cols)\n",
    "    ],\n",
    "    remainder='passthrough'  # Include the rest of the columns as they are\n",
    ")\n",
    "\n",
    "# Define a complete pipeline with a model at the end\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now, you can fit the pipeline on your training data:\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# And transform the data:\n",
    "# X_train_processed = pipeline.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Top N Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TopCategoriesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that groups multi-label categorical features by keeping the top N categories,\n",
    "    replacing any other category with a specified label (e.g., \"Others\").\n",
    "    \"\"\"\n",
    "    def __init__(self, column: str, top_n: int, delimiter: str = \",\", others_label: str = \"Others\"):\n",
    "        self.column = column\n",
    "        self.top_n = top_n\n",
    "        self.delimiter = delimiter\n",
    "        self.others_label = others_label\n",
    "        self.top_categories_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Split and explode the column to get individual categories.\n",
    "        exploded = X[self.column].dropna().str.split(rf\"{self.delimiter}\\s*\").explode().str.strip()\n",
    "        counts = exploded.value_counts()\n",
    "        self.top_categories_ = counts.head(self.top_n).index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        def map_categories(cell):\n",
    "            if pd.isna(cell):\n",
    "                return cell\n",
    "            cats = [cat.strip() for cat in cell.split(self.delimiter)]\n",
    "            # Replace categories not in top_categories_ with others_label.\n",
    "            new_cats = [cat if cat in self.top_categories_ else self.others_label for cat in cats]\n",
    "            # Remove duplicates while preserving order.\n",
    "            seen = set()\n",
    "            new_cats = [x for x in new_cats if x not in seen and not seen.add(x)]\n",
    "            return self.delimiter.join(new_cats)\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        X_transformed[self.column] = X_transformed[self.column].apply(map_categories)\n",
    "        return X_transformed\n",
    "\n",
    "# Usage\n",
    "# genre_transformer = Pipeline(steps=[\n",
    "#     ('top_cat', TopCategoriesTransformer(column='genre_names', top_n=20, delimiter=\",\"))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleanded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully saved to ./data\\01_clean_data.csv with separator ','\n"
     ]
    }
   ],
   "source": [
    "# Saving cleaned dataset for future use\n",
    "save_dataframe(data, \"01_clean_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

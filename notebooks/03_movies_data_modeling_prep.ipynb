{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MultiLabelBinarizer, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score, root_mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory config to project root to insure consistency across environments for project specific imports\n",
    "from pyprojroot import here\n",
    "os.chdir(here())\n",
    "\n",
    "# Project specific imports\n",
    "from ayne.utils.io import (\n",
    "    save_dataframe,\n",
    "    load_dataframe,\n",
    "    save_artifacts,\n",
    "    load_artifacts,\n",
    "    save_processed_data,\n",
    "    load_processed_data\n",
    ")\n",
    "from ayne.ml.models.serialize import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    save_pipeline,\n",
    "    load_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "I have set up multiple versions of my dataset, and the section below is used to swap between them for experimentation.\n",
    "\n",
    "The main purpose behind this setup is to run experiments on data that has been imputed with and without the target included in the inputs for the imputation. I also investigate the effect of splitting the data into train and test sets before imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'revenue'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed Data with Target Included\n",
    "\n",
    "This version of the dataset uses all available features during imputation, including the target variable. It is primarily intended for exploratory analysis to understand the impact of including the target variable in the imputation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset using modern Parquet format\n",
    "# data = load_processed_data(\"imputed_data_full.parquet\")\n",
    "\n",
    "# # Data filters\n",
    "# data = data.dropna(subset=['revenue'])\n",
    "# # data = data[data[\"budget_missing\"] == 0]\n",
    "# # data = data[data[\"revenue_missing\"] == 0]\n",
    "# # data = data[data[\"budget_missing\"] == 0 & data[\"revenue_missing\"] == 0]\n",
    "# # data = data[(data[\"budget_missing\"] == 0) & (data[\"revenue_missing\"] == 0)]\n",
    "\n",
    "# # Split X/y\n",
    "# # X = data.drop(columns=[target])\n",
    "# X = data.drop(columns=[target, 'revenue_missing'])\n",
    "# y = data[target]\n",
    "\n",
    "# # Train/Test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed Data with Target Excluded\n",
    "\n",
    "The target variable was excluded from the imputation process to prevent data leakage and ensure a more reliable model evaluation.  \n",
    "\n",
    "Since missing values in the target column were not imputed, they will have to be dropped, reducing the available data for training. This approach prioritizes model integrity over dataset size, avoiding potential bias introduced by artificially inferred target values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset using modern Parquet format\n",
    "# data = load_processed_data(\"imputed_data_no_revenue.parquet\")\n",
    "\n",
    "# # Data filters\n",
    "# data = data.dropna(subset=['revenue'])\n",
    "# # data = data[data[\"budget_missing\"] == 0]\n",
    "# # data = data[data[\"revenue_missing\"] == 0]\n",
    "# # data = data[data[\"budget_missing\"] == 0 & data[\"revenue_missing\"] == 0]\n",
    "# # data = data[(data[\"budget_missing\"] == 0) & (data[\"revenue_missing\"] == 0)]\n",
    "\n",
    "# # Split X/y\n",
    "# X = data.drop(columns=[target])\n",
    "# y = data[target]\n",
    "\n",
    "# # Train/Test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed Data with Pre-Split Train/Test Sets\n",
    "\n",
    "The dataset was first divided into training and testing subsets before performing imputation. \n",
    "\n",
    "This approach ensures that the imputation model is fitted solely on the training data, thereby preventing data leakage and preserving the integrity of model evaluation. Splitting the data before imputation also helps simulate a real-world scenario, ensuring that the imputed values are more robust and generalizable when applied to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-split and imputed training/test datasets\n",
    "# Using modern Parquet format with load_artifacts utility\n",
    "X_train = load_artifacts(\"X_train_imputed.parquet\")\n",
    "X_test = load_artifacts(\"X_test_imputed.parquet\")\n",
    "y_train = load_artifacts(\"y_train.parquet\")\n",
    "y_test = load_artifacts(\"y_test.parquet\")\n",
    "\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLabel Binarizer\n",
    "\n",
    "Custom transformer to handle multi-label categorical features by encoding them into separate binary columns.  \n",
    "\n",
    "This transformer:\n",
    "- Splits values in the specified column using a delimiter (default: \",\").\n",
    "- Replaces spaces with underscores to ensure consistent naming.\n",
    "- Applies `MultiLabelBinarizer` to encode the unique values as binary features.\n",
    "- Preserves feature names with an optional prefix for clarity.\n",
    "  \n",
    "Designed for use within a `ColumnTransformer`, allowing seamless transformation of multiple multi-label features as part of a `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelBinarizerTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer that applies MultiLabelBinarizer to a specified column.\n",
    "    It splits the column values using a delimiter, replaces spaces with underscores,\n",
    "    fits a MultiLabelBinarizer during fit, and transforms new data using the learned classes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    column : str\n",
    "        The name of the column to encode.\n",
    "    delimiter : str, default=\",\"\n",
    "        The delimiter used in the column to separate values.\n",
    "    prefix : str, optional\n",
    "        The prefix for the dummy columns. If None, uses the column name.\n",
    "    \"\"\"\n",
    "    def __init__(self, column, delimiter=\",\", prefix=None):\n",
    "        self.column = column\n",
    "        self.delimiter = delimiter\n",
    "        self.prefix = prefix if prefix is not None else column\n",
    "        self.mlb_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        # Convert the column values into lists\n",
    "        list_col = X[self.column].apply(\n",
    "            lambda x: [i.strip().replace(' ', '_') for i in x.split(self.delimiter)] if pd.notnull(x) else []\n",
    "        )\n",
    "        self.mlb_ = MultiLabelBinarizer()\n",
    "        self.mlb_.fit(list_col)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Convert column values into lists as in fit\n",
    "        list_col = X[self.column].apply(\n",
    "            lambda x: [i.strip().replace(' ', '_') for i in x.split(self.delimiter)] if pd.notnull(x) else []\n",
    "        )\n",
    "        dummies = pd.DataFrame(\n",
    "            self.mlb_.transform(list_col),\n",
    "            columns=[f\"{self.prefix}_{cls}\" for cls in self.mlb_.classes_],\n",
    "            index=X.index\n",
    "        )\n",
    "        # Drop the original column and add the dummies.\n",
    "        X = X.drop(columns=[self.column])\n",
    "        X = pd.concat([X, dummies], axis=1)\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return [f\"{self.prefix}_{cls}\" for cls in self.mlb_.classes_]\n",
    "\n",
    "# To apply this transformer for multiple columns in a ColumnTransformer:\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "mlb_features = ['genre_names', 'production_country_name', 'spoken_languages']\n",
    "\n",
    "transformers = [\n",
    "    (f\"mlb_{feat}\", MultiLabelBinarizerTransformer(column=feat, delimiter=\",\"), [feat])\n",
    "    for feat in mlb_features\n",
    "]\n",
    "\n",
    "mlb = ColumnTransformer(\n",
    "    transformers=transformers,\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "mlb.set_output(transform='pandas')\n",
    "\n",
    "# Apply transformation\n",
    "# transformed_data = ct.fit_transform(data)\n",
    "# if not isinstance(transformed_data, pd.DataFrame):\n",
    "#     transformed_data = pd.DataFrame(transformed_data)\n",
    "    \n",
    "# print(transformed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of numerical coluns\n",
    "num_cols = make_column_selector(dtype_include=['number'])\n",
    "num_cols = num_cols(X_train)\n",
    "\n",
    "# Display list\n",
    "# for col in num_cols:\n",
    "#     print(col)\n",
    "\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up list of categorical coluns\n",
    "cat_cols = make_column_selector(dtype_include=['object'])\n",
    "cat_cols = cat_cols(X_train)\n",
    "\n",
    "# Display list\n",
    "# for col in cat_cols:\n",
    "#     print(col)\n",
    "\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalers and Encoders  \n",
    "\n",
    "ColumnTransformer pipeline to preprocess numerical and categorical features for modeling.  \n",
    "\n",
    "This transformation includes:  \n",
    "- **StandardScaler**: Normalizes numerical features to ensure consistent scale.  \n",
    "- **OneHotEncoder**: Encodes categorical variables as binary columns while ignoring unknown categories.  \n",
    "- **OrdinalEncoder**: Assigns ordered integer values to categorical features, with unknown values handled explicitly.  \n",
    "\n",
    "The transformer is configured to output a pandas DataFrame for seamless integration into the modeling pipeline.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), [\n",
    "            'metascore', 'rotten_tomatoes_rating', 'meta_critic_rating', 'budget', 'tmdb_vote_count', 'tmdb_vote_average', 'runtime_in_min', \n",
    "            'tmdb_popularity', 'imdb_rating', 'imdb_votes', 'total_wins', 'total_noms', 'oscar_wins', 'oscar_noms', 'bafta_wins', 'bafta_noms', \n",
    "            'release_year', 'release_month', 'release_day', 'movie_age', \n",
    "            # 'roi', 'revenue'\n",
    "            ]),\n",
    "        ('OneHotEncoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['age_rating']),\n",
    "        ('OrdinalEncoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=999999), ['first_billing_actor', 'second_billing_actor', 'third_billing_actor', 'main_director', 'main_writer', 'secondary_writer'])\n",
    "    ],\n",
    "    remainder='passthrough', \n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Set output to pandas dataframe\n",
    "modeling_transformer.set_output(transform='pandas')\n",
    "\n",
    "# transformed_data = modeling_transformer.fit_transform(data)\n",
    "# transformed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function transformer to drop unwanted columns.\n",
    "def drop_unwanted_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    return df.drop(columns=columns, errors='ignore')\n",
    "\n",
    "# List of unwanted columns\n",
    "columns_to_drop = ['production_company_name', 'director', 'writer', 'actors', 'title', 'release_date']\n",
    "\n",
    "# initialize dropper\n",
    "dropper = FunctionTransformer(drop_unwanted_columns, kw_args={'columns': columns_to_drop})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying transformation\n",
    "\n",
    "### Setting up the pipeline\n",
    "\n",
    "This pipeline includes:  \n",
    "- **modeling_transformer**: Applies scaling and encoding transformations.  \n",
    "- **mlb**: Binarizes multi-label categorical features.  \n",
    "- **dropper**: Removes unnecessary columns for cleaner data.  \n",
    "\n",
    "The transformed dataset is ready for model training and evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "scaling_pipeline = Pipeline(steps=[\n",
    "    ('modeling_transformer', modeling_transformer),\n",
    "    # ('mlb', mlb_transformer),\n",
    "    ('mlb', mlb),\n",
    "    ('dropper', dropper)\n",
    "])\n",
    "\n",
    "# scaled_data = scaling_pipeline.fit_transform(data)\n",
    "# scaled_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "X_train_scaled = scaling_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data using the same pipeline\n",
    "X_test_scaled = scaling_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Optimization\n",
    "\n",
    "### Evaluation Function\n",
    "\n",
    "The `model_evaluation` function assesses multiple regression models using 5-fold cross-validation.  \n",
    "It computes and reports the mean R² and RMSE scores for each model, providing a comparative analysis of their performance.  \n",
    "\n",
    "- **R² Score**: Measures how well the model explains variance in the target variable.  \n",
    "- **RMSE Score**: Indicates the model's prediction error in the original units of the target variable.  \n",
    "\n",
    "The results are stored in a dictionary and printed in a structured format for easy interpretation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(models: dict, X_train, y_train, w_train=None):\n",
    "    \"\"\"\n",
    "    Evaluates regression models using Cross Validation.\n",
    "    Returns a report with R² and RMSE scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation_report = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # R² Score\n",
    "        r2_scores = cross_val_score(\n",
    "            model, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv=5, \n",
    "            scoring=\"r2\", \n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # RMSE Score\n",
    "        rmse_scores = cross_val_score(\n",
    "            model, X_train, y_train, \n",
    "            cv=5, \n",
    "            scoring=make_scorer(lambda y, y_pred: root_mean_squared_error(y, y_pred)), \n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        evaluation_report[model_name] = {\n",
    "            \"R² Mean\": np.mean(r2_scores),\n",
    "            \"RMSE Mean\": np.mean(rmse_scores)\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"- CV R² scores: {' | '.join([f'{r * 100:.1f}%' for r in r2_scores])}\")\n",
    "        print(f\"- Mean R²: {np.mean(r2_scores) * 100:.1f}%\")\n",
    "        print(\"- -----------------------------------------------------------------------------------------------------------\")\n",
    "        print(f\"- CV RMSE scores: {' | '.join([f'{r:,.0f}$' for r in rmse_scores])}\")\n",
    "        print(f\"- Mean RMSE: {np.mean(rmse_scores):,.0f}$\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    \n",
    "    return evaluation_report\n",
    "\n",
    "models = {\n",
    "    # \"Linear Regression\": LinearRegression(),\n",
    "    # \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=50, random_state=42),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost Regressor\": XGBRegressor(random_state=42),\n",
    "    \"LightGBM Regressor\": LGBMRegressor(random_state=42),\n",
    "    # \"KNeighbors Regressor\": KNeighborsRegressor(),\n",
    "    # \"SVR\": SVR()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run initial evaluation\n",
    "_ = model_evaluation(models, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning Function\n",
    "\n",
    "The `tune_params_regression` function optimizes hyperparameters for a regression model using `GridSearchCV`.  \n",
    "It performs 5-fold cross-validation to identify the best parameter set based on a specified scoring metric.\n",
    "\n",
    "- **MLflow Integration**: Logs model parameters, performance metrics (R², RMSE), and training data artifacts.  \n",
    "- **Experiment Tracking**: Assigns unique experiment and run names for reproducibility.  \n",
    "- **Artifact Storage**: Saves datasets and trained models for further analysis.  \n",
    "- **Performance Reporting**: Displays the best hyperparameters and test set evaluation results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_params_regression(model, X_train, X_test, y_train, y_test, param_grid, scoring='r2'):\n",
    "    \"\"\"\n",
    "    Evaluates a regression model using cross-validation and logs parameters and metrics with MLflow.\n",
    "    Returns the best estimator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator\n",
    "        The regression model instance.\n",
    "    X_train, X_test : DataFrame or array-like\n",
    "        Training and testing features.\n",
    "    y_train, y_test : array-like\n",
    "        Training and testing target values.\n",
    "    param_grid : dict\n",
    "        The grid of parameters for GridSearchCV.\n",
    "    scoring : str, default='r2'\n",
    "        The scoring metric for GridSearchCV.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    best_model : estimator\n",
    "        The best estimator found by GridSearchCV.\n",
    "    \"\"\"\n",
    "    # Define an experiment name using model type and timestamp.\n",
    "    experiment_name = f\"Exp_{model.__class__.__name__}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Define a custom run name and run_id.\n",
    "    run_name = f\"{model.__class__.__name__}_Run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "    run_id = f\"{model.__class__.__name__}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_{str(uuid.uuid4())[:8]}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "\n",
    "        # MlFlow logs\n",
    "        mlflow.log_param(\"model_type\", model.__class__.__name__)\n",
    "\n",
    "        # Save training datasets using modern utilities\n",
    "        save_artifacts(X_train, \"X_train\", format=\"parquet\")\n",
    "        save_artifacts(pd.DataFrame(y_train), \"y_train\", format=\"parquet\")\n",
    "\n",
    "        # Log these files as artifacts to MLflow\n",
    "        mlflow.log_artifact(\"data/artifacts/X_train.parquet\", artifact_path=\"datasets\")\n",
    "        mlflow.log_artifact(\"data/artifacts/y_train.parquet\", artifact_path=\"datasets\")\n",
    "        \n",
    "        # Initialize GridSearchCV for regression.\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit the model on the training data.\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict on the test set.\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Compute R² and RMSE.\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = root_mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics to MLflow.\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_params(grid_search.best_params_)\n",
    "        mlflow.sklearn.log_model(best_model, f\"Tuned_{model.__class__.__name__}\", input_example=X_train.iloc[:20])\n",
    "        \n",
    "        # Save model using modern joblib-based approach\n",
    "        model_metadata = {\n",
    "            \"model_type\": model.__class__.__name__,\n",
    "            \"r2_score\": r2,\n",
    "            \"rmse\": rmse,\n",
    "            \"best_params\": grid_search.best_params_,\n",
    "            \"cv_folds\": 5,\n",
    "            \"scoring\": scoring,\n",
    "            \"n_features\": X_train.shape[1],\n",
    "            \"feature_names\": X_train.columns.tolist() if hasattr(X_train, 'columns') else None\n",
    "        }\n",
    "        model_filename = f\"{model.__class__.__name__}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        save_model(best_model, model_filename, metadata=model_metadata)\n",
    "        \n",
    "        # Print out the results.\n",
    "        r2_formatted = f\"{r2 * 100:.1f}%\"\n",
    "        rmse_formatted = f\"{rmse:,.0f}$\"\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Test Data Results\")\n",
    "        print(f\"- R² Score: {r2_formatted}\")\n",
    "        print(f\"- RMSE: {rmse_formatted}\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Best Parameters:\")\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            print(f\"- {param}: {value}\")\n",
    "        print(\"----------------------------------------\")\n",
    "        print(f\"Model saved to: data/artifacts/models/{model_filename}.joblib\")\n",
    "        \n",
    "        return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "\n",
    "#### Evaluation Function\n",
    "\n",
    "This function, `evaluate_model`, takes a trained regression model along with its test dataset (features and target values) and computes essential performance metrics. It calculates the R² score (converted to a percentage) and the Root Mean Squared Error (RMSE), then prints these results along with the model's name. Additionally, it visualizes the distribution of residuals via a histogram to help assess model errors. Finally, it returns a dictionary containing the R² and RMSE scores for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a trained regression model on test data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The name of the model for display.\n",
    "    model : estimator\n",
    "        The trained regression model.\n",
    "    X_test : DataFrame or array-like\n",
    "        Test features.\n",
    "    y_test : array-like\n",
    "        True target values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing R² and RMSE scores.\n",
    "    \"\"\"\n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    r2 = r2_score(y_test, y_pred) * 100  # Convert to percentage\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)  # Root Mean Squared Error\n",
    "\n",
    "    # Get model name\n",
    "    model_name = model.__class__.__name__\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  - R² Score: {r2:.1f}%\")\n",
    "    print(f\"  - RMSE: {rmse:,.0f}$\\n\")\n",
    "\n",
    "    # Convert to 1D if necessary\n",
    "    y_test = y_test.values.flatten()\n",
    "\n",
    "    # Plot Residuals\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(residuals, kde=True, bins=30)\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"Residual Distribution for {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return {\"R²\": r2, \"RMSE\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances function\n",
    "\n",
    "This function calculates and displays the feature importances of a trained model. It extracts the feature importance values from the model, sorts them in descending order, and plots the top 20 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_importances(model, dataframe):\n",
    "    \"\"\"\n",
    "    Displays and plots the importance of features for a trained model.\n",
    "    \n",
    "    This function extracts the feature importances from the trained model, \n",
    "    sorts them, and visualizes the top 20 most important features using a \n",
    "    horizontal bar chart.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with feature names and their corresponding importance scores\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': dataframe.columns,           # Feature names\n",
    "        'Importance': model.feature_importances_  # Feature importance values from the model\n",
    "    })\n",
    "\n",
    "    # Sort features by importance in descending order\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot the top 20 most important features\n",
    "    plt.figure(figsize=(10, 6))  # Set the plot size\n",
    "    plt.barh(importance_df['Feature'][:20], importance_df['Importance'][:20], color='skyblue')  # Horizontal bar chart for top 20 features\n",
    "    plt.xlabel('Importance')  # Label the x-axis\n",
    "    plt.title(f'Top Feature Importances from {type(model).__name__}')  # Set the plot title with model type\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to display most important features at the top\n",
    "    plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 300, 500],  \n",
    "#     'max_depth': [None, 10, 20, 30],  \n",
    "#     'min_samples_split': [2, 5, 10],  \n",
    "#     'min_samples_leaf': [1, 2, 4],  \n",
    "#     'max_features': ['auto', 'sqrt', 'log2']  \n",
    "# }\n",
    "\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Example: Tuning a Random Forest Regressor\n",
    "# best_rf = tune_params_regression(\n",
    "#     model=rf,\n",
    "#     X_train=X_train_scaled, \n",
    "#     X_test=X_test_scaled,\n",
    "#     y_train=y_train, \n",
    "#     y_test=y_test,\n",
    "#     param_grid=param_grid_rf,\n",
    "#     scoring='r2'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_gbr = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Tune model\n",
    "# best_gbr = tune_params_regression(\n",
    "#     gbr,\n",
    "#     X_train_scaled, \n",
    "#     X_test_scaled,\n",
    "#     y_train, \n",
    "#     y_test,\n",
    "#     param_grid_gbr\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a XGBoost Regressor\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Set parameter grid\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [5, 7, 10, -1],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'num_leaves': [5, 10, 30],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Tune model\n",
    "best_lgbm = tune_params_regression(\n",
    "    lgbm,\n",
    "    X_train_scaled, \n",
    "    X_test_scaled,\n",
    "    y_train, \n",
    "    y_test,\n",
    "    param_grid_lgbm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_model(best_lgbm, X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_feature_importances(best_lgbm, X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading a saved model with metadata\n",
    "# \n",
    "# # Load model and metadata together\n",
    "# model, metadata = load_model(\"data/artifacts/models/LGBMRegressor_20241122_143000.joblib\", load_metadata=True)\n",
    "# \n",
    "# # View model information\n",
    "# print(\"Model Information:\")\n",
    "# print(f\"- Type: {metadata.get('model_class')}\")\n",
    "# print(f\"- R² Score: {metadata.get('r2_score', 0) * 100:.1f}%\")\n",
    "# print(f\"- RMSE: ${metadata.get('rmse', 0):,.0f}\")\n",
    "# print(f\"- Saved at: {metadata.get('saved_at')}\")\n",
    "# print(f\"- Features: {metadata.get('n_features')}\")\n",
    "# \n",
    "# # Use the model for predictions\n",
    "# # predictions = model.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
